{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513-6513] Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems: \n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular to predict the sentiment of movie review, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multiclass classification).\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using (1) unigrams, bigrams and trigrams to obtain vector representations of documents. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks; 1 for each ngram type**); (2) tf.idf (**1 marks**). \n",
    "- Binary Logistic Regression classifiers that will be able to accurately classify movie reviews trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 1. \n",
    "- Multiclass Logistic Regression classifiers that will be able to accurately classify news articles trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 2. \n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function for Task 1 (**3 marks**)\n",
    "    - Minimise the Categorical Cross-entropy loss function for Task 2 (**3 marks**)\n",
    "    - Use L2 regularisation (both tasks) (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous validation loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength)?  (**2 marks; 0.5 for each model in each task**).\n",
    "- After training the LR models, plot the learning process (i.e. training and validation loss in each epoch) using a line plot (**1 mark; 0.5 for both BOW-count and BOW-tfidf LR models in each task**) and discuss if your model overfits/underfits/is about right.\n",
    "- Model interpretability by showing the most important features for each class (i.e. most positive/negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!).  If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks; 0.5 for BOW-count and BOW-tfidf LR models respectively in each task**)\n",
    "\n",
    "\n",
    "### Data - Task 1 \n",
    "\n",
    "The data you will use for Task 1 are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc..\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows (not recommended) to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 20 Mar 2020** and it needs to be submitted via MOLE. Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.095467Z",
     "start_time": "2020-03-20T23:38:37.282072Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.131092Z",
     "start_time": "2020-03-20T23:38:39.104263Z"
    },
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "class FileLoader:\n",
    "    def __init__(self,path,File=None,names=['text','label']):\n",
    "        self.path = self.__load_path(path)\n",
    "        self.names = names\n",
    "        self.__load_data(File)\n",
    "        \n",
    "    def __load_path(self,path):\n",
    "        import os\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            print(\"Path not exist!\")\n",
    "    \n",
    "    def __load_datum(self,files_name):\n",
    "        import os\n",
    "        full_path = self.path +'/'+files_name.strip().strip('/')\n",
    "        if os.path.exists(full_path):\n",
    "            return pd.read_csv(full_path,header=None,names=self.names)\n",
    "        else:\n",
    "            print(\"File not exist!\")\n",
    "            \n",
    "    def __load_data(self,File):\n",
    "        if File==None:\n",
    "            self.train = self.__load_datum('train.csv')\n",
    "            self.test = self.__load_datum('test.csv')\n",
    "            self.dev = self.__load_datum('dev.csv')\n",
    "            print('Finish loading:\\n\\ttrain.csv\\n\\ttest.csv\\n\\tdev.csv')\n",
    "        else:\n",
    "            self.data = self.__load_datum(File)\n",
    "            print('Finish loading:\\n\\t'+File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.373187Z",
     "start_time": "2020-03-20T23:38:39.142633Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading:\n",
      "\ttrain.csv\n",
      "\ttest.csv\n",
      "\tdev.csv\n"
     ]
    }
   ],
   "source": [
    "Data = FileLoader('data_sentiment')\n",
    "data_tr = Data.train\n",
    "data_te = Data.test\n",
    "data_dev = Data.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.423313Z",
     "start_time": "2020-03-20T23:38:39.380706Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i was growing up in 1970s , boys in my sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the muppet movie is the first , and the best m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  note : some may consider portions of the follo...      1\n",
       "1  note : some may consider portions of the follo...      1\n",
       "2  every once in a while you see a film that is s...      1\n",
       "3  when i was growing up in 1970s , boys in my sc...      1\n",
       "4  the muppet movie is the first , and the best m...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.448533Z",
     "start_time": "2020-03-20T23:38:39.434168Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.489344Z",
     "start_time": "2020-03-20T23:38:39.460890Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "def transform(df):\n",
    "    return list(df['text']), df['label'].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.522109Z",
     "start_time": "2020-03-20T23:38:39.495823Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "X_tr_raw, Y_tr = transform(data_tr)\n",
    "X_te_raw, Y_te = transform(data_te)\n",
    "X_dev_raw, Y_dev = transform(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.550683Z",
     "start_time": "2020-03-20T23:38:39.531309Z"
    },
    "cell_style": "split",
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['note : some may consider portions of the following text to be spoilers .  be forewarned .  it\\'s startling to consider that it was only a few years ago that film distributors would worriedly rearrange their summer release schedules in order to give the annual disney animated feature juggernaut a wide berth .  the lion king had just cracked $300 million domestic in gross to become one of the most profitable ventures in film history , continuing to build on a sturdy base left by prior flicks aladdin and beauty and the beast .  since then , though , disney\\'s animated features have shown an unbroken string of diminishing returns , with pocahontas , the hunchback of notre dame , and this year\\'s hercules successively proving less and less potent .  with the once seemingly-impregnable disney stranglehold on the market share suddenly looking mighty vulnerable , and faced with their first serious competition in the animated film market from fox\\'s anastasia , disney has brought xmas home early by dusting off the feature which sparked the modern revival of feature animation , the little mermaid .  while the animation for the film is , as is typically the case for disney films , unquestionably top-notch , the magic in the little mermaid is not its animation , but the wonderful innocence of its story and its rousingly superb music .  the film\\'s storyline is fairly straightforward -- young teen falls for handsome man , father disapproves and assigns hapless chaperone to his daughter , teen disobeys father and goes to desperate lengths to win her man -- except in this case , the chaperone is a crab , the teen is a young mermaid , and the object of her desire is a human prince .  what makes the little mermaid so affecting and so emotionally resonant is the richness and charm of its characters and the sheer clarity and honest simplicity of their emotions .  from the moment mermaid ariel lays her eyes on prince eric , she\\'s resolutely smitten , and she\\'s such a pure and endearing character that one can\\'t help but invest their heart with her .  this simple but touching love story , coupled with a healthy dose of smart humour , makes the little mermaid a remarkably captivating picture .  one of the interesting things about the little mermaid is something which now curiously dates it : the voices cast for its motley crew of characters .  this film was produced just before the distracting concept of using celebrity voices became in vogue , which started to a certain degree with beauty and the beast and was irrevocably exacerbated by robin williams\\' much-heralded turn in aladdin ; by the release of the lion king and henceforth , the majority of characters in the animated films were voiced by celebrities .  while it\\'s understandable that animated features lacking the name-recognition or drawing power of disney ( say , balto\\'s use of kevin bacon and bridget fonda , or even anastasia\\'s showcasing of meg ryan and john cusack ) would be forced to turn to this strategy in order to hype their products , it\\'s unfortunate that even disney has embraced this policy .  do we really need to hear , say , demi moore as esmerelda in the hunchback of notre dame ?  is the film\\'s entertainment value really augmented by hearing a recognizable voice , rather than a voice which best suits the role ?   ( i\\'m not exactly on the edge of my seat for eddie murphy in the upcoming mulan . )  fortunately , the performers who voice the characters in the little mermaid , although perhaps more obscure , are impeccably cast .  chief among them is jodi benson , a 1992 tony nominee for her stage work in crazy for you , who voices the film\\'s heroine ariel to perfection ; with a wonderfully expressive speaking voice full of youthful vigor , and gorgeous singing voice , ms . benson provides a most engaging anchor for the film .   ( she\\'s the only reason i\\'d even consider catching flubber . )  similarly , samuel e . wright is terrific in the showy role of sebastian , the weary guardian crab .  he easily milks his lovable character\\'s comic moments for all they\\'re worth , and his rendering of two of the little mermaid\\'s big tunes -- \" under the sea \" and \" kiss the girl \" -- have become the stuff of legend .  pat carrol is deliciously villainous and vampy as the evil sea-witch ursula , while kenneth mars\\' booming voice conveys the stern yet affectionate authority of ariel\\'s father , king triton .  in large roles and small ( edie mcclurg as dotting busybody carlotta is ideal , and rene auberjonois has great fun with his exuberant french chef ) , the little mermaid is impeccably cast .  of course , the little mermaid will probably be best remembered for its remarkable collection of songs composed by the songwriting team of alan menken ( music ) and howard ashman ( lyrics ) , who had created little shop of horrors and would go on to compose beauty and the beast and aladdin for disney before mr . ashman\\'s untimely death .  not only are mr . menken\\'s tunes unbearably catchy , but mr . ashman\\'s charming lyrics are fully integrated into the film\\'s storyline so that the songs are a virtual extension of the character\\'s dialogue , and consequently work wonderfully within the context of the film .  mr . menken\\'s score for the film is equally top-notch ; the sequence where eric ( voiced by christopher daniel barnes ) and ariel tour his kingdom in a horse-drawn carriage becomes magical and wondrous with mr . menken\\'s fine score .  it appears that most people prefer the delightfully colourful production number for the calypso-styled \" under the sea \" as joyfully crooned by mr . wright , which won the academy award and golden globe awards for best song -- indeed , one of the many little joys in screening the film during its re-release was listening to children scattered throughout the audience singing along with the tune -- but my favourite is ms . benson\\'s heartfelt rendition of the ballad \" part of your world \" , an achingly beautiful tune of yearning and hope ( wonderfully lyricized by mr . ashman ) which , accompanied by the film\\'s most dazzlingly polished animation sequence , packs an emotional wallop which literally brought tears to my eyes .  during the song\\'s reprise , which builds to a crescendo with ariel arching on a rock as a wave crashes in , the cumulative effect is nothing short of breathtaking , and one becomes acutely aware that this single instance is one of the finest in animation history .  as of this writing , november 1997 has come to an end , as has disney\\'s limited 17-day re-release of the little mermaid .  there\\'s no question that the primary motivation for , if not the film\\'s reissue itself , at least its timing , was to reinforce disney\\'s dominance in the animation market and provide direct competition to fox\\'s costly new upstart animation division and their first major venture , anastasia .  in every respect , the re-release of the little mermaid appears to be a success -- the film\\'s 1997 grosses have pushed its cumulative domestic gross over the magic $100 million mark ; the little mermaid proved to have remarkably strong drawing power for a film initially released only eight years ago and in many homes on video , pulling in close to $10 million in its opening weekend ; and although nobody could possibly expect the little mermaid to possibly defeat the aggressively-marketed anastasia in head-to-head competition , it siphoned enough from the fox film\\'s opening weekend totals to keep anastasia from the coveted weekend leader spot , allowing for disney\\'s odious flubber to sweep in on the subsequent week and wrestle the family demographic market share away .  but although disney\\'s motives in the reissue of the little mermaid were self-serving and protectionist , the real winner is the public .  any reason to put this film back into theatres is a good one , and it\\'s a true joy to see this heartwarming gem back on the silver screen .  the little mermaid is the best film to come out of the disney\\'s modern animation renaissance , and one of the greatest animated films ever made .  ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_raw[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.578266Z",
     "start_time": "2020-03-20T23:38:39.559274Z"
    },
    "cell_style": "split",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.606714Z",
     "start_time": "2020-03-20T23:38:39.584645Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.819541Z",
     "start_time": "2020-03-20T23:38:39.792709Z"
    },
    "code_folding": [
     3,
     13
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):\n",
    "    \"\"\"N-gram extraction from a document.\n",
    "\n",
    "    Args:\n",
    "        x_raw: a string corresponding to the raw text of a document\n",
    "        ngram_range: a tuple of two integers denoting the type of ngrams you want\n",
    "            to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "        token_pattern: a string to be used within a regular expression to extract\n",
    "            all tokens. Note that data is already tokenised so you could opt for\n",
    "            a simple white space tokenisation.\n",
    "        stop_words: a list of stop words\n",
    "        vocab: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "    Returns:\n",
    "        A list of terms to the corresponding N-gram. Each part fits one N-gram,\n",
    "        except 1-gramrow. For example:\n",
    "\n",
    "        ['great','movie','watch',\n",
    "        ('great', 'movie'),('movie', 'watch'),\n",
    "        ('great', 'movie', 'watch')]\n",
    "\n",
    "    \"\"\"\n",
    "    def finder(s,n):\n",
    "        if n <= 0 or n > len(s):\n",
    "            raise Exception('n is out of range')\n",
    "        if n == 1:\n",
    "            return s\n",
    "        else:\n",
    "            return map(lambda i: tuple(s[i:i+n]),range(len(s)-n+1))\n",
    "\n",
    "    # Find all words by condition\n",
    "    pattern = re.compile(token_pattern)\n",
    "    term_eligible = [term.lower() for term in pattern.findall(x_raw) if term.lower() not in stop_words]\n",
    "\n",
    "    # Find combinations of different N-grams\n",
    "    x = [term for n in range(ngram_range[0],ngram_range[1]+1) for term in finder(term_eligible,n)]\n",
    "    \n",
    "    if not vocab:    \n",
    "        return x\n",
    "    else:\n",
    "        return [term for term in x if term in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.897959Z",
     "start_time": "2020-03-20T23:38:39.885434Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'movie',\n",
       " 'watch',\n",
       " ('great', 'movie'),\n",
       " ('movie', 'watch'),\n",
       " ('great', 'movie', 'watch')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"this is a great movie to watch\", \n",
    "               ngram_range=(1,3), \n",
    "               stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:39.974821Z",
     "start_time": "2020-03-20T23:38:39.957082Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', ('great', 'movie')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"this is a great movie to watch\", \n",
    "               ngram_range=(1,2), \n",
    "               stop_words=stop_words, \n",
    "               vocab=set(['great',  ('great','movie')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:40.529515Z",
     "start_time": "2020-03-20T23:38:40.507489Z"
    },
    "code_folding": [
     1,
     5,
     16
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    '''1. create a vocabulary of ngrams\n",
    "       2. count the document frequencies of ngrams\n",
    "       3. their raw frequency\n",
    "\n",
    "    Args:\n",
    "        X_raw: a list of strings each corresponding to the raw text of a document\n",
    "        ngram_range: a tuple of two integers denoting the type of ngrams you want\n",
    "            to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "        token_pattern: a string to be used within a regular expression to extract\n",
    "            all tokens. Note that data is already tokenised so you could opt for\n",
    "            a simple white space tokenisation.\n",
    "        stop_words: a list of stop words\n",
    "        min_df: keep ngrams with a minimum document frequency.\n",
    "        keep_topN: keep top-N more frequent ngrams.\n",
    "\n",
    "    Returns:\n",
    "        vocab: a set of the n-grams that will be used as features.\n",
    "        df: a Counter (or dict) that contains ngrams as keys and their corresponding\n",
    "            document frequency as values.\n",
    "        ngram_counts: counts of each ngram in vocab\n",
    "        For example,\n",
    "\n",
    "    '''\n",
    "    bag_of_ngrams = list()\n",
    "    df = Counter()\n",
    "    for line in X_raw:\n",
    "        ngrams = extract_ngrams(line,ngram_range=(1,3),token_pattern=token_pattern,stop_words=stop_words)\n",
    "        bag_of_ngrams += ngrams\n",
    "        df.update(set(ngrams))\n",
    "    df = Counter({k:v for k,v in df.items() if v >=min_df})\n",
    "    ngram_counts = Counter(bag_of_ngrams)\n",
    "    \n",
    "    vocab = [items[0] for items in ngram_counts.most_common() if items[0] in df.keys()][:keep_topN]\n",
    "\n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:49.052498Z",
     "start_time": "2020-03-20T23:38:40.826179Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "['film', 'but', 'one', 'movie', 'not', 'all', 'there', 'like', 'so', 'out', 'about', 'more', 'up', 'when', 'their', 'some', 'just', 'if', 'into', 'him', 'even', 'only', 'no', 'than', 'time', 'good', 'most', 'story', 'would', 'much', 'character', 'also', 'get', 'two', 'well', 'characters', 'other', 'very', 'first', 'see', 'after', 'because', 'way', 'make', 'off', 'plot', 'while', 'had', 'any', 'too', 'little', 'life', 'films', 'does', 'where', 'people', 'then', 'how', 'me', 'really', 'man', 'scene', 'my', 'never', 'bad', 'being', 'over', 'best', 'don', 'scenes', 'doesn', 'many', 'new', 'know', 'director', 'here', 'action', 'such', 'great', 'through', 'movies', 're', 'love', 'another', 'made', 'go', 'big', 'end', 'seems', 'something', 'still', 'back', 'world', 'us', 'work', 'now', 'down', 'before', 'makes', 'however']\n",
      "\n",
      "[('but', 1334), ('one', 1247), ('film', 1231), ('not', 1170), ('all', 1117), ('movie', 1095), ('out', 1080), ('so', 1047), ('there', 1046), ('like', 1043)]\n"
     ]
    }
   ],
   "source": [
    "vocab, df, ngram_counts = get_vocab(X_tr_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:49.112175Z",
     "start_time": "2020-03-20T23:38:49.058947Z"
    },
    "cell_style": "center",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'film',\n",
       " 1: 'but',\n",
       " 2: 'one',\n",
       " 3: 'movie',\n",
       " 4: 'not',\n",
       " 5: 'all',\n",
       " 6: 'there',\n",
       " 7: 'like',\n",
       " 8: 'so',\n",
       " 9: 'out',\n",
       " 10: 'about',\n",
       " 11: 'more',\n",
       " 12: 'up',\n",
       " 13: 'when',\n",
       " 14: 'their',\n",
       " 15: 'some',\n",
       " 16: 'just',\n",
       " 17: 'if',\n",
       " 18: 'into',\n",
       " 19: 'him',\n",
       " 20: 'even',\n",
       " 21: 'only',\n",
       " 22: 'no',\n",
       " 23: 'than',\n",
       " 24: 'time',\n",
       " 25: 'good',\n",
       " 26: 'most',\n",
       " 27: 'story',\n",
       " 28: 'would',\n",
       " 29: 'much',\n",
       " 30: 'character',\n",
       " 31: 'also',\n",
       " 32: 'get',\n",
       " 33: 'two',\n",
       " 34: 'well',\n",
       " 35: 'characters',\n",
       " 36: 'other',\n",
       " 37: 'very',\n",
       " 38: 'first',\n",
       " 39: 'see',\n",
       " 40: 'after',\n",
       " 41: 'because',\n",
       " 42: 'way',\n",
       " 43: 'make',\n",
       " 44: 'off',\n",
       " 45: 'plot',\n",
       " 46: 'while',\n",
       " 47: 'had',\n",
       " 48: 'any',\n",
       " 49: 'too',\n",
       " 50: 'little',\n",
       " 51: 'life',\n",
       " 52: 'films',\n",
       " 53: 'does',\n",
       " 54: 'where',\n",
       " 55: 'people',\n",
       " 56: 'then',\n",
       " 57: 'how',\n",
       " 58: 'me',\n",
       " 59: 'really',\n",
       " 60: 'man',\n",
       " 61: 'scene',\n",
       " 62: 'my',\n",
       " 63: 'never',\n",
       " 64: 'bad',\n",
       " 65: 'being',\n",
       " 66: 'over',\n",
       " 67: 'best',\n",
       " 68: 'don',\n",
       " 69: 'scenes',\n",
       " 70: 'doesn',\n",
       " 71: 'many',\n",
       " 72: 'new',\n",
       " 73: 'know',\n",
       " 74: 'director',\n",
       " 75: 'here',\n",
       " 76: 'action',\n",
       " 77: 'such',\n",
       " 78: 'great',\n",
       " 79: 'through',\n",
       " 80: 'movies',\n",
       " 81: 're',\n",
       " 82: 'love',\n",
       " 83: 'another',\n",
       " 84: 'made',\n",
       " 85: 'go',\n",
       " 86: 'big',\n",
       " 87: 'end',\n",
       " 88: 'seems',\n",
       " 89: 'something',\n",
       " 90: 'still',\n",
       " 91: 'back',\n",
       " 92: 'world',\n",
       " 93: 'us',\n",
       " 94: 'work',\n",
       " 95: 'now',\n",
       " 96: 'down',\n",
       " 97: 'before',\n",
       " 98: 'makes',\n",
       " 99: 'however',\n",
       " 100: 'few',\n",
       " 101: 'every',\n",
       " 102: 'between',\n",
       " 103: 'seen',\n",
       " 104: 'better',\n",
       " 105: 'enough',\n",
       " 106: 'though',\n",
       " 107: 'real',\n",
       " 108: 'why',\n",
       " 109: 'things',\n",
       " 110: 'going',\n",
       " 111: 'around',\n",
       " 112: 'both',\n",
       " 113: 'year',\n",
       " 114: 'take',\n",
       " 115: 'audience',\n",
       " 116: 'your',\n",
       " 117: 'same',\n",
       " 118: 'role',\n",
       " 119: 'john',\n",
       " 120: 'gets',\n",
       " 121: 'should',\n",
       " 122: 'isn',\n",
       " 123: 'old',\n",
       " 124: 'think',\n",
       " 125: 've',\n",
       " 126: 'look',\n",
       " 127: 'performance',\n",
       " 128: 'long',\n",
       " 129: 'may',\n",
       " 130: 'nothing',\n",
       " 131: 'years',\n",
       " 132: 'actually',\n",
       " 133: 'comedy',\n",
       " 134: 'thing',\n",
       " 135: 'last',\n",
       " 136: 'fact',\n",
       " 137: 'right',\n",
       " 138: 'almost',\n",
       " 139: 'cast',\n",
       " 140: 'say',\n",
       " 141: 'own',\n",
       " 142: 'funny',\n",
       " 143: 'ever',\n",
       " 144: 'although',\n",
       " 145: 'come',\n",
       " 146: 'played',\n",
       " 147: 'script',\n",
       " 148: 'since',\n",
       " 149: 'young',\n",
       " 150: 'find',\n",
       " 151: 'star',\n",
       " 152: 'comes',\n",
       " 153: 'plays',\n",
       " 154: 'actors',\n",
       " 155: 'screen',\n",
       " 156: 'show',\n",
       " 157: 'part',\n",
       " 158: 'again',\n",
       " 159: 'original',\n",
       " 160: 'without',\n",
       " 161: 'effects',\n",
       " 162: 'lot',\n",
       " 163: 'takes',\n",
       " 164: 'day',\n",
       " 165: 'point',\n",
       " 166: 'acting',\n",
       " 167: 'guy',\n",
       " 168: 'interesting',\n",
       " 169: 'himself',\n",
       " 170: 'goes',\n",
       " 171: 'might',\n",
       " 172: 'least',\n",
       " 173: 'high',\n",
       " 174: 'quite',\n",
       " 175: 'during',\n",
       " 176: 'three',\n",
       " 177: 'each',\n",
       " 178: 'minutes',\n",
       " 179: 'far',\n",
       " 180: 'once',\n",
       " 181: 'away',\n",
       " 182: 'place',\n",
       " 183: 'course',\n",
       " 184: 'family',\n",
       " 185: 'set',\n",
       " 186: 'rather',\n",
       " 187: 'anything',\n",
       " 188: 'watch',\n",
       " 189: 'making',\n",
       " 190: 'fun',\n",
       " 191: 'must',\n",
       " 192: 'our',\n",
       " 193: 'special',\n",
       " 194: 'yet',\n",
       " 195: 'didn',\n",
       " 196: 'kind',\n",
       " 197: 'll',\n",
       " 198: 'bit',\n",
       " 199: 'always',\n",
       " 200: ('more', 'than'),\n",
       " 201: 'seem',\n",
       " 202: 'sense',\n",
       " 203: 'give',\n",
       " 204: 'picture',\n",
       " 205: 'series',\n",
       " 206: 'hard',\n",
       " 207: 'wife',\n",
       " 208: 'black',\n",
       " 209: 'want',\n",
       " 210: 'times',\n",
       " 211: 'home',\n",
       " 212: 'job',\n",
       " 213: 'hollywood',\n",
       " 214: 'along',\n",
       " 215: 'woman',\n",
       " 216: 'pretty',\n",
       " 217: 'actor',\n",
       " 218: 'trying',\n",
       " 219: 'instead',\n",
       " 220: 'dialogue',\n",
       " 221: 'everything',\n",
       " 222: 'probably',\n",
       " 223: 'sure',\n",
       " 224: 'help',\n",
       " 225: 'american',\n",
       " 226: 'having',\n",
       " 227: 'men',\n",
       " 228: 'half',\n",
       " 229: 'given',\n",
       " 230: 'becomes',\n",
       " 231: 'gives',\n",
       " 232: 'looking',\n",
       " 233: 'become',\n",
       " 234: 'feel',\n",
       " 235: 'money',\n",
       " 236: 'less',\n",
       " 237: 'horror',\n",
       " 238: 'wants',\n",
       " 239: 'done',\n",
       " 240: 'got',\n",
       " 241: 'together',\n",
       " 242: 'watching',\n",
       " 243: 'girl',\n",
       " 244: 'sex',\n",
       " 245: 'everyone',\n",
       " 246: 'music',\n",
       " 247: 'play',\n",
       " 248: 'whole',\n",
       " 249: 'perhaps',\n",
       " 250: 'father',\n",
       " 251: 'especially',\n",
       " 252: 'human',\n",
       " 253: 'completely',\n",
       " 254: 'moments',\n",
       " 255: 'whose',\n",
       " 256: 'put',\n",
       " 257: 'reason',\n",
       " 258: 'line',\n",
       " 259: 'james',\n",
       " 260: 'case',\n",
       " 261: 'death',\n",
       " 262: 'anyone',\n",
       " 263: 'next',\n",
       " 264: 'itself',\n",
       " 265: 'mind',\n",
       " 266: 'ending',\n",
       " 267: 'entire',\n",
       " 268: 'city',\n",
       " 269: 'true',\n",
       " 270: 'different',\n",
       " 271: 'looks',\n",
       " 272: 'rest',\n",
       " 273: 'until',\n",
       " 274: 'humor',\n",
       " 275: 'michael',\n",
       " 276: 'thought',\n",
       " 277: 'mother',\n",
       " 278: 'small',\n",
       " 279: 'let',\n",
       " 280: 'several',\n",
       " 281: 'simply',\n",
       " 282: 'lost',\n",
       " 283: 'problem',\n",
       " 284: 'soon',\n",
       " 285: 'based',\n",
       " 286: 'written',\n",
       " 287: 'else',\n",
       " 288: 'shows',\n",
       " 289: 'left',\n",
       " 290: 'called',\n",
       " 291: 'friend',\n",
       " 292: 'dead',\n",
       " 293: 'couple',\n",
       " 294: 'main',\n",
       " 295: 'getting',\n",
       " 296: 'performances',\n",
       " 297: 'second',\n",
       " 298: 'night',\n",
       " 299: 'use',\n",
       " 300: 'head',\n",
       " 301: ('special', 'effects'),\n",
       " 302: 'school',\n",
       " 303: 'idea',\n",
       " 304: 'tv',\n",
       " 305: 'evil',\n",
       " 306: 'house',\n",
       " 307: 'found',\n",
       " 308: 'top',\n",
       " 309: 'name',\n",
       " 310: 'comic',\n",
       " 311: 'named',\n",
       " 312: 'either',\n",
       " 313: 'wrong',\n",
       " 314: 'full',\n",
       " 315: 'stars',\n",
       " 316: 'alien',\n",
       " 317: ('there', 'no'),\n",
       " 318: 'often',\n",
       " 319: 'turns',\n",
       " 320: 'unfortunately',\n",
       " 321: 'david',\n",
       " 322: 'later',\n",
       " 323: 'final',\n",
       " 324: 'someone',\n",
       " 325: 'town',\n",
       " 326: 'believe',\n",
       " 327: 'works',\n",
       " 328: 'against',\n",
       " 329: 'begins',\n",
       " 330: 'person',\n",
       " 331: 'certainly',\n",
       " 332: 'sequence',\n",
       " 333: 'used',\n",
       " 334: 'behind',\n",
       " 335: 'live',\n",
       " 336: 'tries',\n",
       " 337: 'run',\n",
       " 338: 'group',\n",
       " 339: 'friends',\n",
       " 340: 'hand',\n",
       " 341: 'boy',\n",
       " 342: 'summer',\n",
       " 343: 'nice',\n",
       " 344: 'seeing',\n",
       " 345: 'game',\n",
       " 346: 'face',\n",
       " 347: 'war',\n",
       " 348: 'shot',\n",
       " 349: 'book',\n",
       " 350: 'turn',\n",
       " 351: 'playing',\n",
       " 352: 'tell',\n",
       " 353: 'despite',\n",
       " 354: 'kids',\n",
       " 355: 'maybe',\n",
       " 356: 'keep',\n",
       " 357: 'hour',\n",
       " 358: ('not', 'only'),\n",
       " 359: 'doing',\n",
       " 360: 'under',\n",
       " 361: 'style',\n",
       " 362: 'finally',\n",
       " 363: 'able',\n",
       " 364: 'relationship',\n",
       " 365: 'disney',\n",
       " 366: 'worth',\n",
       " 367: 'side',\n",
       " 368: 'past',\n",
       " 369: 'said',\n",
       " 370: 'children',\n",
       " 371: 'including',\n",
       " 372: 'days',\n",
       " 373: 'directed',\n",
       " 374: 'need',\n",
       " 375: 'won',\n",
       " 376: 'team',\n",
       " 377: 'short',\n",
       " 378: 'entertaining',\n",
       " 379: 'supposed',\n",
       " 380: 'earth',\n",
       " 381: 'lives',\n",
       " 382: 'starts',\n",
       " 383: 'lines',\n",
       " 384: 'perfect',\n",
       " 385: 'car',\n",
       " 386: 'finds',\n",
       " 387: 'white',\n",
       " 388: 'mr',\n",
       " 389: 'beautiful',\n",
       " 390: 'self',\n",
       " 391: 'running',\n",
       " 392: 'video',\n",
       " 393: ('but', 'not'),\n",
       " 394: 'dark',\n",
       " 395: 'camera',\n",
       " 396: 'early',\n",
       " 397: 'son',\n",
       " 398: 'voice',\n",
       " 399: 'opening',\n",
       " 400: 'joe',\n",
       " 401: 'nearly',\n",
       " 402: 'moment',\n",
       " 403: 'example',\n",
       " 404: 'writer',\n",
       " 405: 'exactly',\n",
       " 406: 'production',\n",
       " 407: 'version',\n",
       " 408: 'computer',\n",
       " 409: 'problems',\n",
       " 410: ('does', 'not'),\n",
       " 411: 'violence',\n",
       " 412: 'fine',\n",
       " 413: 'matter',\n",
       " 414: 'care',\n",
       " 415: 'fight',\n",
       " 416: 'kevin',\n",
       " 417: 'review',\n",
       " 418: 'five',\n",
       " 419: 'try',\n",
       " 420: 'daughter',\n",
       " 421: 'lee',\n",
       " 422: 'hit',\n",
       " 423: 'save',\n",
       " 424: 'heart',\n",
       " 425: 'throughout',\n",
       " 426: 'title',\n",
       " 427: 'simple',\n",
       " 428: 'known',\n",
       " 429: 'sequences',\n",
       " 430: 'eyes',\n",
       " 431: 'start',\n",
       " 432: 'worst',\n",
       " 433: 'coming',\n",
       " 434: 'themselves',\n",
       " 435: 'major',\n",
       " 436: 'obvious',\n",
       " 437: 'order',\n",
       " 438: 'kill',\n",
       " 439: 'deep',\n",
       " 440: 'beginning',\n",
       " 441: 'supporting',\n",
       " 442: 'drama',\n",
       " 443: ('so', 'much'),\n",
       " 444: 'jackie',\n",
       " 445: 'upon',\n",
       " 446: ('each', 'other'),\n",
       " 447: 'jack',\n",
       " 448: 'already',\n",
       " 449: 'robert',\n",
       " 450: 'aren',\n",
       " 451: 'space',\n",
       " 452: 'question',\n",
       " 453: 'guys',\n",
       " 454: 'screenplay',\n",
       " 455: 'killer',\n",
       " 456: 'classic',\n",
       " 457: 'truly',\n",
       " 458: 'sort',\n",
       " 459: 'boring',\n",
       " 460: 'thriller',\n",
       " 461: 'child',\n",
       " 462: 'close',\n",
       " 463: 'others',\n",
       " 464: 'saw',\n",
       " 465: 'women',\n",
       " 466: 'fiction',\n",
       " 467: 'dog',\n",
       " 468: 'act',\n",
       " 469: 'direction',\n",
       " 470: 'sets',\n",
       " 471: 'ship',\n",
       " 472: 'roles',\n",
       " 473: 'happens',\n",
       " 474: 'attempt',\n",
       " 475: 'room',\n",
       " 476: 'note',\n",
       " 477: 'genre',\n",
       " 478: 'tom',\n",
       " 479: 'body',\n",
       " 480: 'four',\n",
       " 481: 'yes',\n",
       " 482: 'wasn',\n",
       " 483: 'quickly',\n",
       " 484: 'stop',\n",
       " 485: 'husband',\n",
       " 486: 'knows',\n",
       " 487: 'laugh',\n",
       " 488: 'van',\n",
       " 489: 'worse',\n",
       " 490: 'novel',\n",
       " 491: 'level',\n",
       " 492: 'ends',\n",
       " 493: 'strong',\n",
       " 494: 'piece',\n",
       " 495: 'tells',\n",
       " 496: 'george',\n",
       " 497: 'hero',\n",
       " 498: 'murder',\n",
       " 499: 'brother',\n",
       " 500: 'god',\n",
       " 501: 'york',\n",
       " 502: 'wonder',\n",
       " 503: 'released',\n",
       " 504: 'particularly',\n",
       " 505: 'sometimes',\n",
       " 506: 'romantic',\n",
       " 507: 'says',\n",
       " 508: 'aliens',\n",
       " 509: 'wild',\n",
       " 510: 'hell',\n",
       " 511: 'fans',\n",
       " 512: 'involving',\n",
       " 513: 'single',\n",
       " 514: 'possible',\n",
       " 515: 'sound',\n",
       " 516: 'none',\n",
       " 517: 'manages',\n",
       " 518: 'feature',\n",
       " 519: 'hope',\n",
       " 520: 'stupid',\n",
       " 521: 'paul',\n",
       " 522: 'career',\n",
       " 523: 'king',\n",
       " 524: 'involved',\n",
       " 525: 'bring',\n",
       " 526: 'impressive',\n",
       " 527: 'attention',\n",
       " 528: 'living',\n",
       " 529: 'lead',\n",
       " 530: 'material',\n",
       " 531: 'planet',\n",
       " 532: 'fall',\n",
       " 533: 'seemed',\n",
       " 534: 'chris',\n",
       " 535: 'experience',\n",
       " 536: 'interest',\n",
       " 537: 'talk',\n",
       " 538: 'lack',\n",
       " 539: 'taking',\n",
       " 540: 'de',\n",
       " 541: 'taken',\n",
       " 542: 'appears',\n",
       " 543: 'basically',\n",
       " 544: 'elements',\n",
       " 545: 'extremely',\n",
       " 546: 'mostly',\n",
       " 547: ('new', 'york'),\n",
       " 548: 'except',\n",
       " 549: 'enjoy',\n",
       " 550: 'talent',\n",
       " 551: 'peter',\n",
       " 552: 'scream',\n",
       " 553: 'theater',\n",
       " 554: 'deal',\n",
       " 555: 'crew',\n",
       " 556: 'meet',\n",
       " 557: 'result',\n",
       " 558: 'recent',\n",
       " 559: 'feeling',\n",
       " 560: 'form',\n",
       " 561: 'power',\n",
       " 562: 'among',\n",
       " 563: 'emotional',\n",
       " 564: 'future',\n",
       " 565: 'usually',\n",
       " 566: 'police',\n",
       " 567: 'light',\n",
       " 568: 'falls',\n",
       " 569: 'success',\n",
       " 570: 'hours',\n",
       " 571: 'oscar',\n",
       " 572: 'across',\n",
       " 573: 'battle',\n",
       " 574: 'smith',\n",
       " 575: 'score',\n",
       " 576: 'late',\n",
       " 577: 'television',\n",
       " 578: 'entertainment',\n",
       " 579: 'parents',\n",
       " 580: 'obviously',\n",
       " 581: 'batman',\n",
       " 582: 'words',\n",
       " 583: 'science',\n",
       " 584: 'stuff',\n",
       " 585: 'cool',\n",
       " 586: 'ways',\n",
       " 587: 'alone',\n",
       " 588: 'mean',\n",
       " 589: 'giving',\n",
       " 590: 'release',\n",
       " 591: 'features',\n",
       " 592: 'number',\n",
       " 593: 'chance',\n",
       " 594: 'feels',\n",
       " 595: 'told',\n",
       " 596: 'tale',\n",
       " 597: 'co',\n",
       " 598: 'viewer',\n",
       " 599: 'important',\n",
       " 600: ('even', 'though'),\n",
       " 601: ('too', 'much'),\n",
       " 602: 'jones',\n",
       " 603: 'wonderful',\n",
       " 604: 'robin',\n",
       " 605: 'forced',\n",
       " 606: 'mission',\n",
       " 607: 'eventually',\n",
       " 608: 'type',\n",
       " 609: 'jokes',\n",
       " 610: 'suspense',\n",
       " 611: 'meets',\n",
       " 612: 'girls',\n",
       " 613: 'williams',\n",
       " 614: 'attempts',\n",
       " 615: 'killed',\n",
       " 616: 'history',\n",
       " 617: 'serious',\n",
       " 618: 'premise',\n",
       " 619: 'guess',\n",
       " 620: 'within',\n",
       " 621: 'needs',\n",
       " 622: 'william',\n",
       " 623: 'america',\n",
       " 624: 'apparently',\n",
       " 625: 'using',\n",
       " 626: 'due',\n",
       " 627: 'ben',\n",
       " 628: 'kid',\n",
       " 629: 'whether',\n",
       " 630: 'happy',\n",
       " 631: 'art',\n",
       " 632: 'middle',\n",
       " 633: 'credits',\n",
       " 634: 'wars',\n",
       " 635: 'expect',\n",
       " 636: 'presence',\n",
       " 637: 'girlfriend',\n",
       " 638: 'went',\n",
       " 639: 'change',\n",
       " 640: 'office',\n",
       " 641: 'easily',\n",
       " 642: 'somehow',\n",
       " 643: 'oh',\n",
       " 644: 'poor',\n",
       " 645: 'word',\n",
       " 646: 'flick',\n",
       " 647: 'wouldn',\n",
       " 648: 'eye',\n",
       " 649: 'laughs',\n",
       " 650: ('rather', 'than'),\n",
       " 651: 'working',\n",
       " 652: 'leads',\n",
       " 653: 'die',\n",
       " 654: 'straight',\n",
       " 655: 'surprise',\n",
       " 656: 'anyway',\n",
       " 657: 'dr',\n",
       " 658: 'easy',\n",
       " 659: 'happen',\n",
       " 660: 'am',\n",
       " 661: 'cop',\n",
       " 662: 'strange',\n",
       " 663: 'ago',\n",
       " 664: 'talking',\n",
       " 665: 'difficult',\n",
       " 666: 'cut',\n",
       " 667: 'remember',\n",
       " 668: 'quality',\n",
       " 669: ('star', 'wars'),\n",
       " 670: 'harry',\n",
       " 671: 'leave',\n",
       " 672: 'whom',\n",
       " 673: 'budget',\n",
       " 674: 'animated',\n",
       " 675: 'slow',\n",
       " 676: 'absolutely',\n",
       " 677: 'smart',\n",
       " 678: 'effective',\n",
       " 679: ('ve', 'seen'),\n",
       " 680: 'cinema',\n",
       " 681: 'crime',\n",
       " 682: 'minute',\n",
       " 683: 'million',\n",
       " 684: ('one', 'most'),\n",
       " 685: 'myself',\n",
       " 686: 'jim',\n",
       " 687: 'definitely',\n",
       " 688: ('year', 'old'),\n",
       " 689: 'surprisingly',\n",
       " 690: 'local',\n",
       " 691: 'runs',\n",
       " 692: 'business',\n",
       " 693: 'personal',\n",
       " 694: 'villain',\n",
       " 695: 'present',\n",
       " 696: 'visual',\n",
       " 697: 'mystery',\n",
       " 698: 'nature',\n",
       " 699: 'age',\n",
       " 700: 'reality',\n",
       " 701: 'return',\n",
       " 702: 'figure',\n",
       " 703: 'blood',\n",
       " 704: 'parts',\n",
       " 705: 'scary',\n",
       " 706: 'filmmakers',\n",
       " 707: 'sequel',\n",
       " 708: 'certain',\n",
       " 709: 'intelligent',\n",
       " 710: 'potential',\n",
       " 711: 'starring',\n",
       " 712: 'somewhat',\n",
       " 713: 'call',\n",
       " 714: 'annoying',\n",
       " 715: 'writing',\n",
       " 716: 'general',\n",
       " 717: 'begin',\n",
       " 718: 'hilarious',\n",
       " 719: 'read',\n",
       " 720: 'stories',\n",
       " 721: 'situation',\n",
       " 722: 'motion',\n",
       " 723: ('high', 'school'),\n",
       " 724: 'third',\n",
       " 725: 'carter',\n",
       " 726: 'uses',\n",
       " 727: 'complete',\n",
       " 728: 'towards',\n",
       " 729: ('but', 'also'),\n",
       " 730: 'fast',\n",
       " 731: 'low',\n",
       " 732: 'box',\n",
       " 733: 'brothers',\n",
       " 734: 'follow',\n",
       " 735: 'romance',\n",
       " 736: ('no', 'one'),\n",
       " 737: 'similar',\n",
       " 738: 'sexual',\n",
       " 739: 'decent',\n",
       " 740: 'rock',\n",
       " 741: 'learn',\n",
       " 742: 'red',\n",
       " 743: 'secret',\n",
       " 744: 'force',\n",
       " 745: 'couldn',\n",
       " 746: 'gone',\n",
       " 747: 'came',\n",
       " 748: 'familiar',\n",
       " 749: 'water',\n",
       " 750: 'actress',\n",
       " 751: 'create',\n",
       " 752: 'successful',\n",
       " 753: 'project',\n",
       " 754: 'add',\n",
       " 755: 'thinking',\n",
       " 756: 'latest',\n",
       " 757: 'dramatic',\n",
       " 758: 'popular',\n",
       " 759: 'shots',\n",
       " 760: 'predictable',\n",
       " 761: 'decides',\n",
       " 762: 'company',\n",
       " 763: 'previous',\n",
       " 764: 'herself',\n",
       " 765: 'near',\n",
       " 766: 'filled',\n",
       " 767: 'giant',\n",
       " 768: 'bill',\n",
       " 769: 'park',\n",
       " 770: 'following',\n",
       " 771: 'events',\n",
       " 772: 'law',\n",
       " 773: 'huge',\n",
       " 774: ('but', 'there'),\n",
       " 775: 'solid',\n",
       " 776: 'message',\n",
       " 777: 'leaving',\n",
       " 778: 'means',\n",
       " 779: 'murphy',\n",
       " 780: 'nor',\n",
       " 781: 'brings',\n",
       " 782: 'brilliant',\n",
       " 783: 'turned',\n",
       " 784: 'clear',\n",
       " 785: ('film', 'but'),\n",
       " 786: 'beyond',\n",
       " 787: 'large',\n",
       " 788: 'bruce',\n",
       " 789: 'former',\n",
       " 790: 'points',\n",
       " 791: 'effect',\n",
       " 792: 'mark',\n",
       " 793: 'clever',\n",
       " 794: 'immediately',\n",
       " 795: 'martin',\n",
       " 796: 'opens',\n",
       " 797: 'move',\n",
       " 798: 'tarzan',\n",
       " 799: 'brought',\n",
       " 800: 'cold',\n",
       " 801: ('even', 'more'),\n",
       " 802: 'rich',\n",
       " 803: 'bob',\n",
       " 804: 'ryan',\n",
       " 805: 'doubt',\n",
       " 806: 'ones',\n",
       " 807: 'woody',\n",
       " 808: 'perfectly',\n",
       " 809: ('much', 'more'),\n",
       " 810: 'amazing',\n",
       " 811: 'moving',\n",
       " 812: 'audiences',\n",
       " 813: 'questions',\n",
       " 814: 'bunch',\n",
       " 815: 'animation',\n",
       " 816: 'chan',\n",
       " 817: 'powerful',\n",
       " 818: 'break',\n",
       " 819: 'usual',\n",
       " 820: 'party',\n",
       " 821: 'merely',\n",
       " 822: 'society',\n",
       " 823: 'likely',\n",
       " 824: 'gun',\n",
       " 825: 'prison',\n",
       " 826: 'understand',\n",
       " 827: 'scott',\n",
       " 828: 'seriously',\n",
       " 829: 'plan',\n",
       " 830: 'sweet',\n",
       " 831: 'dream',\n",
       " 832: 'above',\n",
       " 833: 'mess',\n",
       " 834: 'mars',\n",
       " 835: 'appear',\n",
       " 836: 'steve',\n",
       " 837: 'country',\n",
       " 838: 'non',\n",
       " 839: 'chase',\n",
       " 840: 'viewers',\n",
       " 841: 'ultimately',\n",
       " 842: 'keeps',\n",
       " 843: 'enjoyable',\n",
       " 844: 'key',\n",
       " 845: 'class',\n",
       " 846: 'trouble',\n",
       " 847: 'fails',\n",
       " 848: 'favorite',\n",
       " 849: 'frank',\n",
       " 850: 'happened',\n",
       " 851: 'leaves',\n",
       " 852: 'free',\n",
       " 853: 'silly',\n",
       " 854: 'impossible',\n",
       " 855: 'trek',\n",
       " 856: ('better', 'than'),\n",
       " 857: 'created',\n",
       " 858: 'heard',\n",
       " 859: 'west',\n",
       " 860: 'english',\n",
       " 861: 'subject',\n",
       " 862: 'element',\n",
       " 863: ('very', 'good'),\n",
       " 864: 'view',\n",
       " 865: 'truth',\n",
       " 866: 'sam',\n",
       " 867: 'felt',\n",
       " 868: ('even', 'if'),\n",
       " 869: 'cameron',\n",
       " 870: 'focus',\n",
       " 871: 'excellent',\n",
       " 872: ('their', 'own'),\n",
       " 873: 'agent',\n",
       " 874: 'effort',\n",
       " 875: 'realize',\n",
       " 876: 'haven',\n",
       " 877: 'unlike',\n",
       " 878: 'wanted',\n",
       " 879: 'depth',\n",
       " 880: 'talented',\n",
       " 881: 'stay',\n",
       " 882: 'sit',\n",
       " 883: 'otherwise',\n",
       " 884: 'heavy',\n",
       " 885: 'today',\n",
       " 886: 'fan',\n",
       " 887: 'air',\n",
       " 888: 'escape',\n",
       " 889: 'follows',\n",
       " 890: 'state',\n",
       " 891: ('scene', 'where'),\n",
       " 892: ('there', 'some'),\n",
       " 893: 'complex',\n",
       " 894: 'richard',\n",
       " 895: 'ex',\n",
       " 896: ('so', 'many'),\n",
       " 897: 'vampire',\n",
       " 898: 'exciting',\n",
       " 899: 'neither',\n",
       " 900: 'particular',\n",
       " 901: 'liked',\n",
       " 902: 'showing',\n",
       " 903: 'spend',\n",
       " 904: 'open',\n",
       " 905: 'entirely',\n",
       " 906: 'saying',\n",
       " 907: 'queen',\n",
       " 908: 'studio',\n",
       " 909: 'subtle',\n",
       " 910: ('turns', 'out'),\n",
       " 911: 'ridiculous',\n",
       " 912: 'joke',\n",
       " 913: 'overall',\n",
       " 914: 'quick',\n",
       " 915: 'inside',\n",
       " 916: 'stand',\n",
       " 917: 'jackson',\n",
       " 918: 'tim',\n",
       " 919: 'background',\n",
       " 920: 'humans',\n",
       " 921: 'whatever',\n",
       " 922: 'slightly',\n",
       " 923: 'bond',\n",
       " 924: 'dumb',\n",
       " 925: 'took',\n",
       " 926: 'purpose',\n",
       " 927: 'ten',\n",
       " 928: 'allen',\n",
       " 929: 'wedding',\n",
       " 930: 'various',\n",
       " 931: 'sister',\n",
       " 932: 'drug',\n",
       " 933: 'spent',\n",
       " 934: 'memorable',\n",
       " 935: 'six',\n",
       " 936: 'female',\n",
       " 937: 'situations',\n",
       " 938: 'ability',\n",
       " 939: 'moves',\n",
       " 940: 'nick',\n",
       " 941: 'tone',\n",
       " 942: 'earlier',\n",
       " 943: 'ideas',\n",
       " 944: 'further',\n",
       " 945: 'rated',\n",
       " 946: 'amusing',\n",
       " 947: 'produced',\n",
       " 948: 'college',\n",
       " 949: 'stone',\n",
       " 950: 'dull',\n",
       " 951: 'cannot',\n",
       " 952: 'dreams',\n",
       " 953: 'married',\n",
       " 954: ('other', 'than'),\n",
       " 955: 'totally',\n",
       " 956: 'reasons',\n",
       " 957: 'cinematography',\n",
       " 958: 'british',\n",
       " 959: 'longer',\n",
       " 960: 'double',\n",
       " 961: 'sci',\n",
       " 962: 'fi',\n",
       " 963: ('sci', 'fi'),\n",
       " 964: 'max',\n",
       " 965: 'terrible',\n",
       " 966: 'government',\n",
       " 967: 'highly',\n",
       " 968: 'thin',\n",
       " 969: 'typical',\n",
       " 970: ('but', 'still'),\n",
       " 971: 'political',\n",
       " 972: 'climax',\n",
       " 973: ('science', 'fiction'),\n",
       " 974: 'setting',\n",
       " 975: 'suddenly',\n",
       " 976: 'fairly',\n",
       " 977: 'hear',\n",
       " 978: 'convincing',\n",
       " 979: 'street',\n",
       " 980: 'fire',\n",
       " 981: 'army',\n",
       " 982: 'ask',\n",
       " 983: 'brief',\n",
       " 984: 'actual',\n",
       " 985: 'expected',\n",
       " 986: 'hold',\n",
       " 987: 'hate',\n",
       " 988: ('one', 'best'),\n",
       " 989: 'waste',\n",
       " 990: 'modern',\n",
       " 991: 'central',\n",
       " 992: 'approach',\n",
       " 993: 'recently',\n",
       " 994: 'pay',\n",
       " 995: 'private',\n",
       " 996: 'rating',\n",
       " 997: 'minor',\n",
       " 998: 'atmosphere',\n",
       " 999: 'intelligence',\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_dict = dict(enumerate(vocab))\n",
    "reference_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:52.956700Z",
     "start_time": "2020-03-20T23:38:49.124804Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "X_tr_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_tr_raw]\n",
    "X_te_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_te_raw]\n",
    "X_dev_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_dev_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:38:52.976529Z",
     "start_time": "2020-03-20T23:38:52.960905Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    '''1. select the features of vocab from X_ngram.\n",
    "       2. convert X_ngram into matrix\n",
    "\n",
    "    Args:\n",
    "        X_ngram(list of list): a list of texts (documents) features(Bag-of-ngram)\n",
    "        vocab(list): a set of selected features(n-grams)\n",
    "\n",
    "    Returns:\n",
    "        X_vec: an array shapes (#document,#vocab), where document is a single line\n",
    "            in dataset.\n",
    "    '''\n",
    "\n",
    "    X_vec = np.zeros([len(X_ngram),len(vocab)])\n",
    "    for docs_index in range(len(X_ngram)):\n",
    "        temp = Counter(X_ngram[docs_index])\n",
    "        for feature_index in range(len(vocab)):\n",
    "            X_vec[docs_index,feature_index] = temp.get(vocab[feature_index],0)\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.707444Z",
     "start_time": "2020-03-20T23:38:52.992606Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "X_tr_count = vectorise(X_tr_ngram, vocab)\n",
    "X_te_count = vectorise(X_te_ngram, vocab)\n",
    "X_dev_count = vectorise(X_dev_ngram, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.731328Z",
     "start_time": "2020-03-20T23:39:04.713318Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.766139Z",
     "start_time": "2020-03-20T23:39:04.739489Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.,  6.,  8.,  0.,  4.,  1.,  1.,  0.,  3.,  1.,  1.,  1.,  0.,\n",
       "         0.,  6.,  1.,  2.,  1.,  2.,  0.,  3.,  4.,  1.,  1.,  0.,  1.,\n",
       "         4.,  2.,  3.,  1.,  3.,  0.,  0.,  1.,  0.,  4.,  0.,  0.,  2.,\n",
       "         1.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  2.,  1.,  0.],\n",
       "       [ 6.,  2.,  5.,  0.,  2.,  4.,  2.,  3.,  3.,  2.,  2.,  3.,  4.,\n",
       "         0.,  0.,  2.,  0.,  2.,  2.,  5.,  0.,  0.,  1.,  3.,  2.,  1.,\n",
       "         2.,  2.,  1.,  1.,  5.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         1.,  0.,  1.,  2.,  1.,  0.,  2.,  3.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count[:2,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.801789Z",
     "start_time": "2020-03-20T23:39:04.785947Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def get_idfs(vocab,D):\n",
    "    idfs = np.zeros(len(vocab))\n",
    "    for i in range(len(vocab)):\n",
    "        idfs[i] = df[vocab[i]]\n",
    "    idfs = np.log10(D/idfs)\n",
    "    #idfs = np.log(D/idfs)\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.837472Z",
     "start_time": "2020-03-20T23:39:04.813444Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05586998, 0.02097221, 0.05026158, ..., 1.84509804, 1.82390874,\n",
       "       1.86737443])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = get_idfs(vocab,len(X_tr_raw))\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.920767Z",
     "start_time": "2020-03-20T23:39:04.843161Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "X_tr_tfidf = X_tr_count*idfs\n",
    "X_te_tfidf = X_te_count*idfs\n",
    "X_dev_tfidf = X_dev_count*idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.953215Z",
     "start_time": "2020-03-20T23:39:04.932392Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3352199 , 0.04194441, 0.25130791, 0.        , 0.15588435,\n",
       "       0.39229945, 0.2531927 , 0.38353118, 0.37854406, 0.22540856,\n",
       "       0.28361332, 0.38728409, 0.55011146, 0.        , 0.        ,\n",
       "       0.32320144, 0.        , 0.34692489, 0.36468042, 1.24525516,\n",
       "       0.        , 0.        , 0.21927133, 0.55839959, 0.40923321,\n",
       "       0.22967409, 0.43239695, 0.52266534, 0.25626631, 0.24195367,\n",
       "       1.37569611, 0.26818108, 0.26076682, 0.2699102 , 0.        ,\n",
       "       0.        , 0.25794854, 0.        , 0.        , 0.30664999,\n",
       "       0.        , 0.34472433, 0.59341724, 0.29609478, 0.        ,\n",
       "       0.67313664, 0.94469503, 0.        , 0.32395996, 0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf[1,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:04.984708Z",
     "start_time": "2020-03-20T23:39:04.962474Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # fill in your code...\n",
    "    z = 1 / (1 + np.exp(np.minimum(-z,709.782)))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.014411Z",
     "start_time": "2020-03-20T23:39:04.992498Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.00669285 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0)) \n",
    "print(sigmoid(np.array([-5., 1.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.034098Z",
     "start_time": "2020-03-20T23:39:05.022887Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    preds_proba = X.dot(weights.T)\n",
    "    \n",
    "    assert preds_proba.shape == (X.shape[0],1)\n",
    "    return sigmoid(preds_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.070652Z",
     "start_time": "2020-03-20T23:39:05.054291Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    preds_proba = predict_proba(X, weights)\n",
    "    preds_class = np.where(preds_proba>=0.5,1,0)\n",
    "    \n",
    "    assert preds_class.shape == (X.shape[0],1)\n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.131687Z",
     "start_time": "2020-03-20T23:39:05.084229Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    \n",
    "    # fill in your code...\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    preds_proba = predict_proba(X, weights)\n",
    "    l = -1*(Y*np.log(preds_proba+eps) + (1-Y)*np.log(1-preds_proba+eps)) + alpha*np.multiply(weights, weights)\n",
    "    l = np.mean(l)\n",
    "    assert l.shape == (1,1) or type(l) == np.float64\n",
    "    return l#.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.156643Z",
     "start_time": "2020-03-20T23:39:05.141566Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, weights, alpha=0.00001):\n",
    "    m = len(X)\n",
    "    h = predict_proba(X, weights)\n",
    "    dw = np.dot((h - y).T,X) + 2*alpha*weights\n",
    "    assert dw.shape == weights.shape\n",
    "    return dw/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:05.202421Z",
     "start_time": "2020-03-20T23:39:05.167768Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], loss=\"binary\", lr=0.1, alpha=0.00001, epochs=5, tolerance=0.0001, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    # fill in your code...\n",
    "    \n",
    "    # Stage 1: Init weights\n",
    "    #weights = np.random.rand(1,X_tr.shape[1])\n",
    "    weights = np.zeros((1,X_tr.shape[1]))\n",
    "    \n",
    "    # Stage 2: Init stochastic value\n",
    "    idx_list = np.array(range(X_tr.shape[0]))\n",
    "    \n",
    "    # Stage 3: Training\n",
    "    for epoch in range(epochs):\n",
    "        loss_tr = 0\n",
    "        np.random.shuffle(idx_list) # disorder dataset\n",
    "        for i in idx_list:\n",
    "            X_tr_i, Y_tr_i = X_tr[i].reshape(1,-1),Y_tr[i].reshape(1,-1) # get a single data pair\n",
    "            dw = gradient(X_tr_i, Y_tr_i, weights, alpha=0.00001) # gradient\n",
    "            weights -= lr*dw # update\n",
    "            loss_tr += binary_loss(X_tr_i, Y_tr_i, weights, alpha) # add loss\n",
    "        \n",
    "        loss_dev = binary_loss(X_dev_count, Y_dev, weights, alpha)\n",
    "        loss_tr /= len(idx_list)\n",
    "        # Add history\n",
    "        if epoch != 0:\n",
    "            training_loss_history.append(loss_tr)\n",
    "            validation_loss_history.append(loss_dev)\n",
    "        if print_progress == True: \n",
    "            print(\"Epoch: %d| Training loss: %f| Validation loss: %f\"%(epoch,loss_tr,loss_dev))\n",
    "        if epoch >1 and (validation_loss_history[-2]-validation_loss_history[-1]) <= tolerance:\n",
    "            break\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:38.140876Z",
     "start_time": "2020-03-20T23:39:05.210601Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Training loss: 0.647481| Validation loss: 0.645617\n",
      "Epoch: 1| Training loss: 0.596317| Validation loss: 0.615235\n",
      "Epoch: 2| Training loss: 0.558449| Validation loss: 0.591136\n",
      "Epoch: 3| Training loss: 0.529314| Validation loss: 0.573161\n",
      "Epoch: 4| Training loss: 0.504013| Validation loss: 0.558668\n",
      "Epoch: 5| Training loss: 0.484747| Validation loss: 0.546929\n",
      "Epoch: 6| Training loss: 0.466581| Validation loss: 0.535233\n",
      "Epoch: 7| Training loss: 0.451016| Validation loss: 0.527671\n",
      "Epoch: 8| Training loss: 0.437302| Validation loss: 0.517705\n",
      "Epoch: 9| Training loss: 0.424444| Validation loss: 0.511145\n",
      "Epoch: 10| Training loss: 0.412677| Validation loss: 0.506243\n",
      "Epoch: 11| Training loss: 0.402548| Validation loss: 0.497909\n",
      "Epoch: 12| Training loss: 0.392776| Validation loss: 0.492174\n",
      "Epoch: 13| Training loss: 0.383561| Validation loss: 0.487693\n",
      "Epoch: 14| Training loss: 0.375044| Validation loss: 0.482096\n",
      "Epoch: 15| Training loss: 0.367154| Validation loss: 0.477879\n",
      "Epoch: 16| Training loss: 0.359281| Validation loss: 0.473862\n",
      "Epoch: 17| Training loss: 0.353003| Validation loss: 0.470283\n",
      "Epoch: 18| Training loss: 0.346436| Validation loss: 0.465987\n",
      "Epoch: 19| Training loss: 0.340209| Validation loss: 0.462874\n",
      "Epoch: 20| Training loss: 0.334284| Validation loss: 0.459601\n",
      "Epoch: 21| Training loss: 0.328634| Validation loss: 0.456856\n",
      "Epoch: 22| Training loss: 0.323118| Validation loss: 0.454146\n",
      "Epoch: 23| Training loss: 0.318026| Validation loss: 0.451016\n",
      "Epoch: 24| Training loss: 0.312674| Validation loss: 0.450296\n",
      "Epoch: 25| Training loss: 0.308494| Validation loss: 0.446179\n",
      "Epoch: 26| Training loss: 0.303604| Validation loss: 0.443928\n",
      "Epoch: 27| Training loss: 0.299425| Validation loss: 0.441737\n",
      "Epoch: 28| Training loss: 0.295194| Validation loss: 0.439569\n",
      "Epoch: 29| Training loss: 0.290503| Validation loss: 0.439210\n",
      "Epoch: 30| Training loss: 0.287264| Validation loss: 0.435747\n",
      "Epoch: 31| Training loss: 0.283382| Validation loss: 0.433939\n",
      "Epoch: 32| Training loss: 0.279527| Validation loss: 0.433188\n",
      "Epoch: 33| Training loss: 0.276133| Validation loss: 0.430679\n",
      "Epoch: 34| Training loss: 0.272720| Validation loss: 0.429141\n",
      "Epoch: 35| Training loss: 0.269420| Validation loss: 0.427586\n",
      "Epoch: 36| Training loss: 0.266214| Validation loss: 0.426098\n",
      "Epoch: 37| Training loss: 0.262985| Validation loss: 0.424736\n",
      "Epoch: 38| Training loss: 0.260010| Validation loss: 0.423446\n",
      "Epoch: 39| Training loss: 0.256987| Validation loss: 0.422133\n",
      "Epoch: 40| Training loss: 0.254177| Validation loss: 0.420881\n",
      "Epoch: 41| Training loss: 0.251313| Validation loss: 0.419672\n",
      "Epoch: 42| Training loss: 0.248625| Validation loss: 0.418550\n",
      "Epoch: 43| Training loss: 0.245954| Validation loss: 0.417476\n",
      "Epoch: 44| Training loss: 0.243264| Validation loss: 0.416399\n",
      "Epoch: 45| Training loss: 0.240877| Validation loss: 0.415367\n",
      "Epoch: 46| Training loss: 0.238314| Validation loss: 0.414559\n",
      "Epoch: 47| Training loss: 0.235957| Validation loss: 0.413605\n",
      "Epoch: 48| Training loss: 0.233580| Validation loss: 0.412639\n",
      "Epoch: 49| Training loss: 0.231317| Validation loss: 0.411803\n",
      "Epoch: 50| Training loss: 0.229102| Validation loss: 0.410848\n",
      "Epoch: 51| Training loss: 0.226919| Validation loss: 0.410068\n",
      "Epoch: 52| Training loss: 0.224740| Validation loss: 0.409228\n",
      "Epoch: 53| Training loss: 0.222616| Validation loss: 0.408403\n",
      "Epoch: 54| Training loss: 0.220567| Validation loss: 0.407830\n",
      "Epoch: 55| Training loss: 0.218529| Validation loss: 0.407219\n",
      "Epoch: 56| Training loss: 0.216735| Validation loss: 0.406248\n",
      "Epoch: 57| Training loss: 0.214748| Validation loss: 0.405565\n",
      "Epoch: 58| Training loss: 0.212892| Validation loss: 0.404887\n",
      "Epoch: 59| Training loss: 0.211005| Validation loss: 0.404230\n",
      "Epoch: 60| Training loss: 0.209169| Validation loss: 0.403677\n",
      "Epoch: 61| Training loss: 0.207373| Validation loss: 0.403053\n",
      "Epoch: 62| Training loss: 0.205589| Validation loss: 0.402488\n",
      "Epoch: 63| Training loss: 0.203926| Validation loss: 0.401968\n",
      "Epoch: 64| Training loss: 0.202142| Validation loss: 0.401494\n",
      "Epoch: 65| Training loss: 0.200557| Validation loss: 0.400948\n",
      "Epoch: 66| Training loss: 0.198986| Validation loss: 0.400362\n",
      "Epoch: 67| Training loss: 0.197355| Validation loss: 0.399857\n",
      "Epoch: 68| Training loss: 0.195886| Validation loss: 0.399275\n",
      "Epoch: 69| Training loss: 0.194343| Validation loss: 0.398790\n",
      "Epoch: 70| Training loss: 0.192731| Validation loss: 0.398505\n",
      "Epoch: 71| Training loss: 0.191191| Validation loss: 0.398196\n",
      "Epoch: 72| Training loss: 0.189974| Validation loss: 0.397449\n",
      "Epoch: 73| Training loss: 0.188404| Validation loss: 0.397110\n",
      "Epoch: 74| Training loss: 0.186985| Validation loss: 0.396645\n",
      "Epoch: 75| Training loss: 0.185669| Validation loss: 0.396177\n",
      "Epoch: 76| Training loss: 0.184313| Validation loss: 0.395796\n",
      "Epoch: 77| Training loss: 0.182922| Validation loss: 0.395456\n",
      "Epoch: 78| Training loss: 0.181655| Validation loss: 0.395057\n",
      "Epoch: 79| Training loss: 0.180362| Validation loss: 0.394685\n",
      "Epoch: 80| Training loss: 0.179073| Validation loss: 0.394333\n",
      "Epoch: 81| Training loss: 0.177825| Validation loss: 0.393991\n",
      "Epoch: 82| Training loss: 0.176514| Validation loss: 0.393782\n",
      "Epoch: 83| Training loss: 0.175319| Validation loss: 0.393367\n",
      "Epoch: 84| Training loss: 0.173989| Validation loss: 0.393420\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, Y_tr, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=Y_dev, \n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:38.586427Z",
     "start_time": "2020-03-20T23:39:38.148728Z"
    },
    "cell_style": "center",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xUZdr/8c+VTnoPISGNTkJoAUGQoqKABdsKCNa1brGtru6zz+q6bvHZn+sidnAtq66IWFcRFaRLLyIlQGhJCCEF0glp9++PM4QQk5BAJpNkrvfrNa+ZOXPmzMUB5jv3fc65bzHGoJRSynm5OLoApZRSjqVBoJRSTk6DQCmlnJwGgVJKOTkNAqWUcnIaBEop5eQ0CFSHIyKuIlIiIjGtuW57JCK3ishXrbi9Dr0/lH1oECi7s33xnLrViMiJOs9ntHR7xphqY4yvMSa9NddtKRH5s4gYEflFveWP2Jb/7/l+hjHmbWPMJNt23WzbjTuP7dltf6iOS4NA2Z3ti8fXGOMLpANX1Vn2Xv31RcSt7as8Z3uAW+stu9m2vF3pYPtVtSENAuVwtl/WH4jI+yJSDMwUkZEislZECkTkiIjMFhF32/pn/DIWkXdtr38lIsUiskZE4lu6ru31SSKyR0QKReQFEVktIrc1Uf4aIFhE+tjePwjr/9WWen/Ge0UkTUTyReRTEYmsV989ttePi8jsOu+7U0SW2Z6usN3vsLWmrm/mtn8hImlAahvsD9UBaRCo9uJa4D9AAPABUAU8AIQCo4CJwD1NvP8m4A9AMFar4+mWrisi4cB84FHb5x4Ahjej9neAW2yPbwH+XfdFEbkM+BNwAxAFZAH1W0KTgaHAYKwgvLSBzxlju0+0taY+aua2rwaGAQMaqb+194fqYDQIVHuxyhjzX2NMjTHmhDFmgzFmnTGmyhizH5gDjG3i/QuMMRuNMZVYX4SDzmHdK4GtxpjPbK/9E8hrRu3vADNsLZYb+ekX8QzgdWPMVmNMOfA4MFZEouus8zdjTKEx5iCw7Cz1t3TbfzXGHDfGnGhkG629P1QHo0Gg2ouMuk9EpK+IfCki2SJShPWrN7SJ92fXeVwG+J7Dut3q1mGsERkzz1a4MeYA1i/pvwI7jDFZ9VbpBhyqs34RcBzrF/y51N/SbWfUf1M9rbo/VMejQaDai/rD4L4GbAd6GmP8gScAsXMNR4DaX9IiIpz5hdqUfwO/oV63kE0WEFtnu35AEHC4hfU1NFRwc7Z9rkMMn8/+UB2IBoFqr/yAQqBURPrR9PGB1vIFMERErrKdYfMAENbM9/4HuAz4qIHX3gd+LiLJIuIJ/A1YaYxp0a9rY0w1kA8ktPa2G3E++0N1IBoEqr36DdZpmcVYrYMP7P2BxpijwFTgOawv3B5YZ/+cbMZ7y4wxi2399PVfW4TVtfUJ1q/sGKy+/XPxJPAf29lU17Xyts9wPvtDdSyiE9Mo1TARccXqernBGLPS0fU4mu6PzktbBErVISITRSTA1s3yB6zTWNc7uCyH0f3hHDQIlDrTaGA/1mmSE4FrjDHO3BWi+8MJaNeQUko5OW0RKKWUk+twg1CFhoaauLg4R5ehlFIdyqZNm/KMMQ2e/tvhgiAuLo6NGzc6ugyllOpQRORQY69p15BSSjk5DQKllHJyGgRKKeXkOtwxAqVU26usrCQzM5Py8p+MoKHaGS8vL6Kjo3F3d2/2ezQIlFJnlZmZiZ+fH3FxcViDkKr2yBhDfn4+mZmZxMfHn/0NNto1pJQ6q/LyckJCQjQE2jkRISQkpMUtNw0CpVSzaAh0DOfy9+Q8QZCTCot+B1UVjq5EKaXaFecJgoJ0WPsy7PvO0ZUopVqooKCAl19++ZzeO3nyZAoKCppc54knnmDx4sXntP364uLiyMvrWFM7O08QJIwDr0DY8bGjK1FKtVBTQVBdXd3kexcuXEhgYGCT6/zpT3/i0ksvPef6OjrnCQI3D+h3FaQuhEo9BU6pjuTxxx9n3759DBo0iEcffZRly5Yxfvx4brrpJgYMGADANddcw9ChQ0lMTGTOnDm17z31C/3gwYP069ePu+66i8TERC677DJOnDgBwG233caCBQtq13/yyScZMmQIAwYMIDU1FYDc3FwmTJjAkCFDuOeee4iNjT3rL//nnnuOpKQkkpKSmDVrFgClpaVcccUVDBw4kKSkJD744IPaP2P//v1JTk7mkUcead0deBbOdfpo0nWw5R1I+9YKBaVUiz313x3szCpq1W327+bPk1clNvr6M888w/bt29m6dSsAy5YtY/369Wzfvr32NMk33niD4OBgTpw4wbBhw7j++usJCQk5Yzt79+7l/fffZ+7cudx444189NFHzJw58yefFxoayubNm3n55Zd59tlnef3113nqqae4+OKL+d3vfseiRYvOCJuGbNq0iTfffJN169ZhjOGCCy5g7Nix7N+/n27duvHll18CUFhYyLFjx/jkk09ITU1FRM7aldXa7NoisM1utFtE0kTk8UbWGSciW0Vkh4gst2c9xI0B71DYrt1DSnV0w4cPP+Nc+dmzZzNw4EBGjBhBRkYGe/fu/cl74uPjGTRoEABDhw7l4MGDDW77uuuu+8k6q1atYtq0aQBMnDiRoKCgJutbtWoV1157LT4+Pvj6+nLdddexcuVKBgwYwOLFi3nsscdYuXIlAQEB+Pv74+XlxZ133snHH3+Mt7d3S3fHebFbi8A2v+lLwAQgE9ggIp8bY3bWWScQeBmYaIxJF5Fwe9UDgKsb9L8afpgHFaXg4WPXj1OqM2rql3tb8vE5/f932bJlLF68mDVr1uDt7c24ceMaPJfe09Oz9rGrq2tt11Bj67m6ulJVVQVYF2u1RGPr9+7dm02bNrFw4UJ+97vfcdlll/HEE0+wfv16lixZwrx583jxxRf57ru2O7HFni2C4UCaMWa/MaYCmAdMqbfOTcDHxph0AGNMjh3rsSReB5VlsOdru3+UUqp1+Pn5UVxc3OjrhYWFBAUF4e3tTWpqKmvXrm31GkaPHs38+fMB+Oabbzh+/HiT648ZM4ZPP/2UsrIySktL+eSTT7jooovIysrC29ubmTNn8sgjj7B582ZKSkooLCxk8uTJzJo1q7YLrK3Y8xhBFJBR53kmcEG9dXoD7iKyDPADnjfG/Lv+hkTkbuBugJiYmHMuqKCsgoCYkYhvhHX2UNJ157wtpVTbCQkJYdSoUSQlJTFp0iSuuOKKM16fOHEir776KsnJyfTp04cRI0a0eg1PPvkk06dP54MPPmDs2LFERkbi5+fX6PpDhgzhtttuY/jw4QDceeedDB48mK+//ppHH30UFxcX3N3deeWVVyguLmbKlCmUl5djjOGf//xnq9ffJGOMXW7Az4DX6zy/GXih3jovAmsBHyAU2Av0bmq7Q4cONefi0y2ZJvaxL8zBvBJjvnzEmKfDjSkvOqdtKeVsdu7c6egSHK68vNxUVlYaY4z5/vvvzcCBAx1cUeMa+vsCNppGvlft2SLIBLrXeR4NZDWwTp4xphQoFZEVwEBgT2sX0yvcSu6tGQXEJl4H6+fA7q8g+cbW/iilVCeUnp7OjTfeSE1NDR4eHsydO9fRJbUaewbBBqCXiMQDh4FpWMcE6voMeFFE3AAPrK4ju7SJekf40sXdlS3pBUxJvgD8usH2jzQIlFLN0qtXL7Zs2eLoMuzCbgeLjTFVwK+Ar4FdwHxjzA4RuVdE7rWtswtYBGwD1mN1JW23Rz1uri4kRwewJaMAXFxg4FTY+w0cO2CPj1NKqQ7DrtcRGGMWGmN6G2N6GGP+Ylv2qjHm1Trr/D9jTH9jTJIxZpY96xkUE8iurCJOVlXD8HtAXK3xh5RSyok5zxATwODugVRU17Ajqwj8I61uoS3vQtkxR5emlFIO41xBEGNdCbg13Xb59shfWdcUbPiXA6tSSinHcqogiPD3IjLAi60ZtiCI6A89J8D613QgOqU6GV9fXwCysrK44YYbGlxn3LhxbNy4scntzJo1i7KystrnzRnWujn++Mc/8uyzz573dlqDUwUBwKDugWzJqHNF4IW/htJc+OF9xxWllLKbbt261Y4sei7qB0FzhrXuaJwuCAbHBJJx7AT5JSetBfFjIHIgrHkRamocW5xSqkGPPfbYGfMR/PGPf+Qf//gHJSUlXHLJJbVDRn/22Wc/ee/BgwdJSkoC4MSJE0ybNo3k5GSmTp16xlhD9913HykpKSQmJvLkk08C1kB2WVlZjB8/nvHjxwNnTjzT0DDTTQ133ZitW7cyYsQIkpOTufbaa2uHr5g9e3bt0NSnBrxbvnw5gwYNYtCgQQwePLjJoTeay7mGoQYGdbcdJ8go4JJ+ESACF94PH/0cdi+Eflc6uEKl2rmvHofsH1t3m10HwKRnGn152rRpPPjgg/ziF78AYP78+SxatAgvLy8++eQT/P39ycvLY8SIEVx99dWNztv7yiuv4O3tzbZt29i2bRtDhgypfe0vf/kLwcHBVFdXc8kll7Bt2zbuv/9+nnvuOZYuXUpoaOgZ22psmOmgoKBmD3d9yi233MILL7zA2LFjeeKJJ3jqqaeYNWsWzzzzDAcOHMDT07O2O+rZZ5/lpZdeYtSoUZSUlODl5dXs3dwYp2sRDIgKwNVF2JJep4+v/zUQFA/fPQ3VVY4rTinVoMGDB5OTk0NWVhY//PADQUFBxMTEYIzhf/7nf0hOTubSSy/l8OHDHD16tNHtrFixovYLOTk5meTk5NrX5s+fz5AhQxg8eDA7duxg586djW0GaHyYaWj+cNdgDZhXUFDA2LFjAbj11ltZsWJFbY0zZszg3Xffxc3N+t0+atQoHn74YWbPnk1BQUHt8vPhdC2CLh6u9O3qd/qAMVjDU0/4E8y/GTa/BcPudFh9SrV7Tfxyt6cbbriBBQsWkJ2dXdtN8t5775Gbm8umTZtwd3cnLi6uweGn62qotXDgwAGeffZZNmzYQFBQELfddttZt2OaGJa6ucNdn82XX37JihUr+Pzzz3n66afZsWMHjz/+OFdccQULFy5kxIgRLF68mL59+57T9k9xuhYBWAeMf8gooKamzl9kv6sgdjQs/SuUFzquOKVUg6ZNm8a8efNYsGBB7VlAhYWFhIeH4+7uztKlSzl06FCT2xgzZgzvvfceANu3b2fbtm0AFBUV4ePjQ0BAAEePHuWrr76qfU9jQ2A3Nsx0SwUEBBAUFFTbmnjnnXcYO3YsNTU1ZGRkMH78eP7+979TUFBASUkJ+/btY8CAATz22GOkpKTUTqV5PpyuRQBWELy3Lp19uSX0irANIysCl/8F5oyDFc/CZU87tEal1JkSExMpLi4mKiqKyMhIAGbMmMFVV11FSkoKgwYNOusv4/vuu4/bb7+d5ORkBg0aVDtE9MCBAxk8eDCJiYkkJCQwatSo2vfcfffdTJo0icjISJYuXVq7vLFhppvqBmrM22+/zb333ktZWRkJCQm8+eabVFdXM3PmTAoLCzHG8NBDDxEYGMgf/vAHli5diqurK/3792fSpEkt/rz6pKnmTXuUkpJiznbe79mk5RRz6XMr+PsNydyY0v3MFz+5D7YvgF+uh+D4hjeglJPZtWsX/fr1c3QZqpka+vsSkU3GmJSG1nfKrqGEUF/8vNzOPE5wyiVPgIsbLH6y7QtTSikHcMogcHERBnUPZPOhBqaa84+EUQ/Czs9g7+K2L04ppdqYUwYBwMgeIaRmF5NT1MCZAaMegLC+8NkvdUA6pWw6WjeyszqXvyenDYKxvcMAWLE376cvunvBdXOhLB/++wDofwDl5Ly8vMjPz9cwaOeMMeTn57f4IjOnPGsIoH+kP2F+nizfk8sNQ6N/ukJkMlz8e1j8R9j2AQyc1uY1KtVeREdHk5mZSW5urqNLUWfh5eVFdHQD32lNcNogEBHG9ApjSepRqmsMri4NXJJ+4f2w5xtY+CjEXgiBMW1fqFLtgLu7O/HxehZdZ+W0XUMAY/uEUVBWybbMRoaUdXGFa1+1uoY+uRdqqtu2QKWUagNOHQQX9QxFBJbvaaK5GxQLk/8Oh1bD9y+0XXFKKdVGnDoIgnw8GBgd2HQQAAycDv2uhu/+DEe2tU1xSinVRpw6CMA6e+iHjAIKyioaX0kErnoevEPg47t1NjOlVKeiQdAnjBoDq9IaOI20Lu9guOYlyN0FS55qm+KUUqoNOH0QDIwOJKCLO8t3N+O0uJ6XwvB7YO3LsPurs6+vlFIdgNMHgauLcFGvUJbvyW3exTITnrKmtvzgZtj+sf0LVEopO3P6IADrOEFO8UlSs5sx96d7F7jlc4hOgQV3wMY37F+gUkrZkQYBp4ebWLo7p3lv6BIIMz+GXpfBFw9Z8xcopVQHpUEAhPt7kRwdwNc7Gp/r9Cc8vGHae5A81ZrreP1c+xWolFJ2pEFgMzGpKz9kFJBV0IK5RV3d4ZpXoPdE+Oox2L/cfgUqpZSdaBDYTEqypr5btD27ZW90cbVGKg3tBR/eCscO2KE6pZSyHw0Cm/hQH/p29Wt5EAB4+cP0960xid6fDuVFrV+gUkrZiQZBHROTurLh0DFyi0+2/M3BCXDj25C3B+bfomGglOowNAjqmJQUiTHwzc5zaBUAJIyDq2fDgRXw+iWQt7c1y1NKKbvQIKijd4Qv8aE+59Y9dMrgmXDLZ9bsZnMv1iuQlVLtngZBHSLCxKSurNmX3/QgdGcTfxHcvRyC4+H9abDod9pVpJRqtzQI6pmU1JWqGsO3O1twTUFDArvDHV9Dyh2w9hV4MQV++EDnP1ZKtTsaBPUMiAogKrDL+XUPneLeBa78J9y1BPyj4JO74a0roPQsI50qpVQbsmsQiMhEEdktImki8ngDr48TkUIR2Wq7PWHPeprjVPfQyr15FJdXts5Go4bCnUusOQ0Ob4a3r4bS/NbZtlJKnSe7BYGIuAIvAZOA/sB0EenfwKorjTGDbLc/2auelrh6YDcqqmv4dMvh1tuoiwsMvQ1umgfH9sE7U6DsWOttXymlzpE9WwTDgTRjzH5jTAUwD5hix89rNQO7BzIwOoC31xxq3tDULZEwzhqjKHc3vHMtnCho3e0rpVQL2TMIooCMOs8zbcvqGykiP4jIVyKS2NCGRORuEdkoIhtzc5sxgUwruGVkHGk5JaxOs0MXTs9LYep7cHSHdcwgd3frf4ZSSjWTPYNAGlhW/+f1ZiDWGDMQeAH4tKENGWPmGGNSjDEpYWFhrVxmw64cGEmIjwdvfX/QPh/Q+zKrm6j4CLw2Bta9pmcUKaUcwp5BkAl0r/M8Gsiqu4IxpsgYU2J7vBBwF5FQO9bUbJ5urkwfHsOS1KNkHCuzz4f0vBTuWwNxF8FXv4V3r4PCTPt8llJKNcKeQbAB6CUi8SLiAUwDPq+7goh0FRGxPR5uq6fdnE4zY0QMLiK8u/aQ/T7ELwJmfAhX/AMOrYEXhsK3T0J5of0+Uyml6rBbEBhjqoBfAV8Du4D5xpgdInKviNxrW+0GYLuI/ADMBqaZVj86e+4iA7pweWIE8zZkcKKi2n4fJALD7oRfrYf+U2D1LHh+kHUhWnWV/T5XKaUAaUffu82SkpJiNm7c2Gaft25/PlPnrOWZ6wYwbXhM23zokR/gmz/AgeUQlQLXvgahPdvms5VSnZKIbDLGpDT0ml5ZfBbD44Pp29WPt74/2PqnkjYmcqA1cN31/4L8NHh1tHUwuaambT5fKeVUNAjOQkS4Y3Q8qdnFrEprw6EhRGDADfCLtRA32jqY/NZkSF/bdjUopZyCBkEzTBnUjTA/T+audMA0lP6R1sHkq1+A/H3wxuXw7vXWUBVKKdUKNAiawdPNlVtHxrJiTy6p2Q4YTloEhtwCD2yFS5+Cw5tg7nj4zzTrsVJKnQcNgmaacUEsXdxded0RrYJTPHxg9IPwwDYY/3tIX2NNfvPu9ZC+znF1KaU6NA2CZgry8eBnKdF8tvUwOUXlji3Gyx/G/hYe/BEueRKytsAbl8HrE2D7x3rKqVKqRTQIWuCOUfFU1Rj7DTvRUl7+cNHDViBM/D8ozYUFt8PzA+H7F6HyhKMrVEp1ABoELRAX6sPl/bvy3rp0yira0a9uDx8YcS/8ehNMn2dNkfnN72H2YNj4BlS30rwKSqlOSYOghe4aE0/hiUrmrc84+8ptzcUV+kyC276A2xZCYAx88RC8OAzWz4UTxx1doVKqHdIgaKGhscFcEB/My8v2ta9WQX1xo6w5k2/6ELoEwsJH4Nk+8OHtkLYYauw4ZIZSqkPRIDgHv53Yh7ySk7y5+qCjS2maiDXc9d3L4J4V1gxp+5daZxk9PwhW/D8oOuLgIpVSjqZBcA6GxgZzSd9wXlu+j8KyDtL/HjkQJv8dfrMbbngDguPguz/DPxOt6xG2zYdyB1wjoZRyOA2Cc/TI5X0oKq/itRX7HF1Ky7h5QtL1cOt/4debYeQvrUHuPr4L/l9PmDcDtn0IJ4sdXalSqo3o6KPn4f73t/DtzqMs/+04wv28HF3Ouaupgcz1sONT2PmpNWuaqyf0vAQSr4W+V1hnJimlOiwdfdROHp7Qm8rqGl78Ls3RpZwfFxeIGQGTnoGHdloHmVPugKytVkvh2T7w+f2QsUGn01SqE9IWwXn6n09+5MONGXz70FjiQjvZr+aaGmsYi63vwY5PoLIMvEPAtyv4hoFPuHW6av9rrDBRSrVbTbUINAjOU05ROeOfXcYFCSG8cdswR5djPyeLrTDI3AiledZVzAXpUJINYf1g3GPQb4oGglLtVFNB4NbWxXQ24f5ePHhpb/6ycBeLdx7l0v4Rji7JPjz9rBFQh9xyellNtRUOy/8PPrwNwvpCv6uhx8UQnQKu7g4rVynVfNoiaAWV1TVMfn4l5VXVfPvQWLzcXR1dUtuqqbYGu1v/mjUstqkBT3+IHwMJ4yBhPIT0sK5rUEo5hHYNtYHv9+Vx09x1PHhpLx68tLejy3GcE8fhwApIW2JdvFaQbi0P6A4JY61QiB8DvuGOrVMpJ6NdQ23gwh6hXJkcySvL9nH9kGi6B3s7uiTH6BIE/adYN2Pg2H4rEPYthV3/hS3vWuuF9oYuweDhDe7eENoLhtxqDZinlGpT2iJoRUcKT3DJP5ZzYY8Q5t6SgmhXyJlqquHIVti/3OpCOlkMFaXWLW+P1aXU+3IYfpfVcnBxsi42pexIWwRtJDKgCw9e2ou/Lkxl0fZsJg2IdHRJ7YuLK0QNtW71FWXBxjdh05vw7iLw8IOoIRA9zLr3j7K6k3zC9CC0Uq1MWwStrKq6hikvrSan+CSLHx5LQBf90mqRqpOQ+iUcWg0Z6+HoDjD1RkoNireuX+gzGWJGgqv+nlHqbPRgcRv7MbOQKS+tYuqwGP523QBHl9OxVZRCzi4ozobSHCg+Clmbre6l6pPgFQjdh0PXZOg6wLoFxev1DErVo11DbWxAdAB3jIrn9VUHuGZQNy5ICHF0SR2Xh491TUJ9J0tg33ew52srGNKWnG45ePhB1yRbOCRBRBKE9wP3Lm1bu1IdhLYI7KSsoorL/rkCDzcXvnrgIjzd9MCnXVWWQ+4uOLINsn+E7G2QvR0qS63XxQVCelqBENYPwvpYj4N7gJuHY2tXqg1oi8ABvD3c+Mu1A7j1jfXMXrKXRy/v6+iSOjd3L+g22LqdUlMDxw/A0e1WKBzdboXEzs8B2w8gcYWgOOt01rDeENrHConQ3uDl74g/iVJtToPAjsb2DuNnQ6N5Zdk+Lu4bztDYYEeX5FxcXKwrmkN6WNc1nFJ5AvL2Qm6qddpq3h7I3QP7lkB1xen1fLta1zeE9Dx9H9LTmgtaz1xSnYh2DdlZcXklk55fiYsICx+4CF9Pzd52q7oKjh+EvN2Quxvy02xBsRfKC06v5+JmhUFwgnVgOjjeug+KhcBY8PR12B9BqcboWUMOtv7AMabOWcPUlO48c32yo8tRLWUMlOVD/j4rHPL3WldMHztgBcfJelN8eoda3U1BsdZ9YKwVHEGx4B+txySUQ+gxAgcbHh/MPWN68OryfVzSL4IJnXWE0s5KBHxCrVvMBWe+ZgyUHbMCoeCgdX/8IBw/ZF09vePTetdBiHVhnG8E+HW1bgHdISD69M0/yppSVKk2okHQRh6a0Ivle3J5/KNtJEdfRIR/B57aUp0mAj4h1i26gSumq6ug6LA1+F5BOhQcsq6iLjlqXRuRtcWa26E+3wgrFPwiwb+bLTQiNSyUXTSra0hEegCZxpiTIjIOSAb+bYwpaPqdra8jdg2dsudoMVNeXE3fSD/m3T1CTylVlspyKyyKDkNBxungKMywwqL4CJQX/vR9PmFWYJy6nQqLuve+EdoVpYBWOEYgIluBFCAO+Br4HOhjjJncinU2S0cOAoCFPx7hF+9tth0vGKAD06nmqSizAqEw0wqKU/clObbWxVFrtriaqp++1zvU6o7qEmS7Bdrug8E72Jp+1D/KOo7hHaLzRnRSrXGMoMYYUyUi1wKzjDEviMiW1ivReUweEMmvxvfkxaVpJEb5c8vIOEeXpDoCD+/Tp8I2pqbGOqhdfMR2yz7doijNhRMF1gHuE8eseSOqyn+6Dbcu4B9phYdPmNXlVfs41AqKU/feodb1G6rDa24QVIrIdOBW4CrbsrOeSC0iE4HnAVfgdWPMM42sNwxYC0w1xixoZk0d1sMTerPrSBF/+u9Oekf4MUKHoFCtwcUFfMOsW2Qzzk6rKLNCoTQXCm2tjMIMW3DkWQe9MzdY4VJ/4L9T3L2t8Z5qWxlBp1sZXYLrtEJsLRGvAOvm4astj3akuV1D/YF7gTXGmPdFJB7rS7vBL3bbe1yBPcAEIBPYAEw3xuxsYL1vgXLgjbMFQUfvGjqlqLySa15azfHSCj775WhiQpx0IhvV/tXUWNdRlOVbAVGWD2Wn7o9ZLY3yAquVceL46eU1lY1vU1ytebC9/MEz4HRAdAm0BUvQmeFyarlXgDUNqh73aLFWvY5ARIKA7saYbWdZbyTwR2PM5bbnvwMwxvyt3noPApXAMOALZwkCgAN5pVzz0mrC/Tz5+BcX4uelV6uqTsIYa+KhugFx4jiUF1kHvssLrMcni07f17Z6joUAABmVSURBVAZKwekxohrj4fvTgDgVEh7eVheXuxe4eVmDDbp3sVovnn7gE261mrwCnapVct7HCERkGXC1bf2tQK6ILDfGPNzE26KAjDrPM4EzTsIWkSjgWuBirCBo7PPvBu4GiImJaU7JHUJ8qA+vzBjCLW+s59fvb+Fftw7D1cV5/mGqTkzE+rXv5W8dhG6pqoo6AXLMFh6FtiCxhUXdgDm2/3SgVJQ23pVVl6uHFQzuPqenTK3bSql97GcFjKevFUAePta6p+5PPXbz7LDB0txjBAHGmCIRuRN40xjzpIg02SIAGtoj9Zsfs4DHjDHVTZ09Y4yZA8wBq0XQzJo7hAt7hvLUlER+/8l2/vLlLp64qr+jS1LK8dw8wC/Cup2L6kprTKmqcuu+8gRUnbACpDTXOtuqNMc2XWoZVJZZAXKy2HrtZLEVKvWvGm+KuJ4ZFh4+p0Om7uPaELG1Uty8rFBy8wBXT+s1T19rOHU3T2sKV4x17xVoHYNpZc0NAjcRiQRuBH7fzPdkAt3rPI8GsuqtkwLMs4VAKDBZRKqMMZ828zM6hRkXxJKWU8Ibqw+QEObDzBGxji5JqY7N1d02MOB5jiBbUwMVxVZr49T82hUl1n1lmS1kyk4vqyi15sqoLD39vOTomWFTWXbm4IYtMepBmPDU+f2ZGtDcIPgT1vUDq40xG0QkAdh7lvdsAHrZDiwfBqYBN9VdwRgTf+qxiLyFdYzAqULglN9P7sfBvFKe+Gw7Ef5eOgyFUu2Bi8vp4w+tqbrKFiRlVqulutKaprXqZJ1QKbGei4vV5SQuEGaf4eybFQTGmA+BD+s83w9cf5b3VInIr7ACxBXrjKAdInKv7fVXz7nqTsjN1YWXZgxh+py1/Pr9zfznrhEMiQlydFlKKXtwdQNX/3Yz50VzTx+NBl4ARmH1868CHjDGZNq3vJ/qTGcNNSSv5CTXv/I9xeVVLLh3JAlhOqSxUur8NXXWUHNn+H4Ta1iJblhnA/3Xtky1slBfT96+fTgC3PrmenKKGrj6UymlWlFzgyDMGPOmMabKdnsLCLNjXU4tLtSHf902jGMlFcx4fR35JScdXZJSqhNrbhDkichMEXG13WYC+fYszNkN6h7Iv24bRvqxMm7+13oKy5q4SlMppc5Dc4PgDqxTR7OBI8ANwO32KkpZRiSEMOeWFNJySrjlzfUUl2sYKKVaX7OCwBiTboy52hgTZowJN8ZcA1xn59oUMLZ3GC/NGMKOw4Xc/uYGDQOlVKtrbougIU0NL6Fa0YT+EcyePpitGQXM1G4ipVQrO58g6JiDanRQkwdE8vKMIezKKmL63LV6AFkp1WrOJwg61Zg/HcFliV2Ze2sK+3JLmDZnrZ5aqpRqFU0GgYgUi0hRA7dirGsKVBsb2zuMt24fzuGCE0yds5asghOOLkkp1cE1GQTGGD9jjH8DNz9jTHPHKVKtbGSPEN75+XDyik9y42trSM8vc3RJSqkO7Hy6hpQDDY0N5j93jaDkZBU3vraGfbklji5JKdVBaRB0YAOiA3j/rhFU1dQw9bU1bDh4zNElKaU6IA2CDq5fpD/z7h6Jr6cb0+as5fWV+2np9KNKKeemQdAJ9Az35fNfj2ZCvwj+/OUu7n13E0V64ZlSqpk0CDoJfy93Xpk5hP+9oh+Ld+Vw9Qur2HO02NFlKaU6AA2CTkREuPOiBN6/awQlJ6u59qXVLNqe7eiylFLtnAZBJzQ8Ppj//noUPSP8uPfdTTz3zW5qavS4gVKqYRoEnVRkQBc+uHsEPxsazezv0rjj7Q0cLz3HCbOVUp2aBkEn5uXuyt9vSObpa5L4Pi2fK2avZHP6cUeXpZRqZzQIOjkR4eYRsSy4byQuLsKNr67hX6sO6CmmSqlaGgROIjk6kC9/fRHj+oTz9Bc7uevfGzmmXUVKKTQInEqAtztzbxnKE1f2Z8WePCbOWsH3aXmOLksp5WAaBE5GRLhjdDyf/PJCfL3cmPGvdTzzVSoVVTWOLk0p5SAaBE4qsVsAX/x6NNOGdefV5fuY8tJqdmfrBWhKOSMNAifm7eHG365LZu4tKeQWl3PVC6uYs2If1XrNgVJORYNAMaF/BF8/OIZxfcL468JUps9Zy6H8UkeXpZRqIxoECoAQX09eu3ko//jZQHZlFzFx1kr+veagXpGslBPQIFC1RITrh0bzzUNjGB4fzBOf7WDG6+s4mKetA6U6Mw0C9RORAV146/Zh/N/1A9h+uJDLZq3ghSV7OVlV7ejSlFJ2oEGgGiQiTB0Ww+LfjGVC/wj+8e0eJj+/knX78x1dmlKqlWkQqCZF+Hvx0k1DeOv2YVRU1zB1zlr+99MfKdaJb5TqNDQIVLOM6xPONw+O5c7R8by3Lp3L/7mCZbtzHF2WUqoVaBCoZuvi4cr/Xtmfj+67EG9PN257cwP3vLOR1OwiR5emlDoPGgSqxYbEBPHl/aN5eEJvvk/LZ+KslfzyP5tJy9Erk5XqiKSjDUeckpJiNm7c6OgylE1BWQVzV+7nzdUHKa+s5uYRsTx8WR8Curg7ujSlVB0isskYk9LQa3ZtEYjIRBHZLSJpIvJ4A69PEZFtIrJVRDaKyGh71qNaX6C3B49e3peVvx3PzBGxvLP2EBc/u4z5GzP0YjSlOgi7tQhExBXYA0wAMoENwHRjzM466/gCpcYYIyLJwHxjTN+mtqstgvZt++FCnvx8B5sOHWdgdAAPX9aHMb1CERFHl6aUU3NUi2A4kGaM2W+MqQDmAVPqrmCMKTGnk8gH0J+QHVxSVAAf3jOSf/xsIHklFdz6xnp+9uoanfdAqXbMnkEQBWTUeZ5pW3YGEblWRFKBL4E7GtqQiNxt6zramJuba5diVetxcbGGqvjukbE8fU0SmcdPcNPr65g+Zy2bDh1zdHlKqXrsGQQN9QX85Be/MeYTW3fQNcDTDW3IGDPHGJNijEkJCwtr5TKVvXi6uXLziFiWPTqOJ67sz96cYq5/ZQ23v7me7YcLHV2eUsrGnkGQCXSv8zwayGpsZWPMCqCHiITasSblAF7urtwxOp4Vvx3PYxP7sjm9gCtfWMXDH2wlu7Dc0eUp5fTsGQQbgF4iEi8iHsA04PO6K4hIT7EdRRSRIYAHoIPZdFLeHm7cN64HKx8bzy/G9eCLH48w/tllzF6yl/JKHdBOKUdxs9eGjTFVIvIr4GvAFXjDGLNDRO61vf4qcD1wi4hUAieAqaajXdigWszfy53fTuzL9OEx/HXhLp77dg//XnOQG4Z2Z9qw7sSF+ji6RKWcil5Qphxu3f58Xl91gO9Sc6iuMYxMCOG2UXFM6BeBi4uedqpUa2jq9FG7tQiUaq4LEkK4ICGEo0XlLNiUyfvr07nnnU30CvflvnE9uGpgN9xddTQUpexFWwSq3amqruHLH4/wyrJ9pGYXExXYhXvGJnBjSne83F0dXZ5SHVJTLQINAtVuGWP4LjWHl5amsTm9gBAfD+4YHc/MEbE6lpFSLaRBoDo0YwzrDxzj5WX7WL4nF28PV64ZHMXMC2Lp383f0eUp1SHoMQLVoYlI7XGEHVmFvLX6IB9tyuQ/69JJiQ3i5pGxTEqKxMNNjyModS60RaA6pIKyChZsyuTdtYc4mF9GqK8H04bFcNMFMXQL7OLo8pRqd7RrSHVaNTWGlWl5vLPmIEtScxDg4r4RzLgghjG9w3DV00+VArRrSHViLi7C2N5hjO0dRsaxMt5fn878jZks3nWUqMAuTBvWnRtSookM0FaCUo3RFoHqdCqqavh251H+s/4Qq9PycREY1yecacO6M75vuF6ToJySdg0pp3Uov5T5GzP4cGMmOcUnCfPz5GdDo5k2LIaYEG9Hl6dUm9EgUE6vqrqGpbtz+WBDOt+l5lBjYFTPEK4fEs3liV3x8dReUtW5aRAoVceRwhN8uDGT+RszyDx+gi7urkxM6sq1g6MY1TNUDzCrTkmDQKkGGGPYeOg4H28+zBfbsiguryLC35NrBkdxw5BoekX4ObpEpVqNBoFSZ1FeWc13qTl8tCmTZXtyqa4x9I/056qB3bgyOZLuwXo8QXVsGgRKtUBeyUk+35rFf7dlsSW9AIDBMYFcPbAbVyRHEu7n5eAKlWo5DQKlzlHGsTK+2HaEz3/IYteRIlwELuwRylUDI7msf1eCfDwcXaJSzaJBoFQr2Hu0mM9/yOKzrVmkHyvD1UW4sEcIk5IiuTwxghBfT0eXqFSjNAiUakXGGHZkFbHwxyMs/PEIB/PLcBEYHh/M5AGRXJ7YlQh/7T5S7YsGgVJ2Yoxh15FiFm0/wlfbs9mbU4IIpMQGMSkpkolJXXUQPNUuaBAo1Ub2Hi1m4Y/ZfLX9CKnZxQAMjA5gfN9wLu4bTlK3AJ2HWTmEBoFSDrA/t4SvtmezeNdRtmYUYAyE+XlySd9wLkuM4MIeoTr1pmozGgRKOVh+yUlW7M1lya4clu3OpeRkFd4erozrE8b4PuGM7ROmp6Uqu9IgUKodOVlVzdr9x/hmh9VaOFp0EoCkKH/G9wlnXJ9wBnUP1KEuVKvSIFCqnTp1sHnp7hyW7c5hc3oB1TWGIG93a56FPmFc1CuMUD01VZ0nDQKlOojCskpW7M1laWoOy/bkcqy0AoDEbv6M6R3GRT1DGRoXhKebHltQLaNBoFQHVFNjXa+wYm8uy/fksvnQcapqDF3cXRkeH8xFvUIZ1TOUvl39ENFuJNU0DQKlOoGSk1Ws25/Pyr15rNyby77cUgBCfT0Y1dMKhdE9Q/W6BdUgnbNYqU7A19ONS/pFcEm/CMCaV2HV3jxWp+WxKi2fz7ZmAZAQ6mMLhhBGJIQQ6K3jIammaYtAqU7AGMPuo8W1wbDuwDHKKqoRsY4vXNgjlBEJwaTEBePv5e7ocpUDaNeQUk6moqqGbZkFrE7LZ/W+PLamF1BRXYOLQFJUACMSQjQYnIwGgVJOrryyms3px1m7/xhr9+WzNeN0MCR2C2BITCCDY4IYHBNITLC3HnzuhDQIlFJnKK+sZkt6AWv357P+wDF+yCygrKIasA4+D48PZnhcMMPjQ+jT1U8vbusE9GCxUuoMXu6ujOwRwsgeIQBU1xj2HC1mc/pxNh08zroDx1j4YzYAfp5uDIoJZGhsEENjgxgcE4Svp351dCbaIlBKNSjzeBnrDxxj06HjbDp0nN1HizEGXAT6dvUnJc4KhiExQUQHddHupHZOu4aUUuetuLySLekFbDx0nE2HjrEl/XR3UrifJ0NighgSax1rGBAVoCOrtjPaNaSUOm9+Xu6M6R3GmN5hAFRV15CaXcyW9ONsTi9g06HjLNphdSe5uQj9Iv0ZHBNo3boHERuiB6HbK7u2CERkIvA84Aq8box5pt7rM4DHbE9LgPuMMT80tU1tESjVfuWVnGRregGb04+zJb2AbZkFlNpaDYHe7gyICmBAVADJ0QEM7B5IZIBeBd1WHNI1JCKuwB5gApAJbACmG2N21lnnQmCXMea4iEwC/miMuaCp7WoQKNVxVNcY9uYU14bCtsxCdmcXU1Vjfe909fdicEwgg7oHMiAqgMRuAQR463UN9uCorqHhQJoxZr+tiHnAFKA2CIwx39dZfy0Qbcd6lFJtzNVF6NvVn75d/Zk+PAawTl3ddaSIrRkFbEkvYGtGAV9tz659T3RQFxK7+ZPYLYDEbv4kRQUQ7uep3Up2ZM8giAIy6jzPBJr6tf9z4KuGXhCRu4G7AWJiYlqrPqWUA3i5u9ouXgvi9lHWsvySk+zIKmJHVhHbswrZmVXE1zuO1r4n1NeTAVH+DIgKICkqgH6R/nqmUiuyZxA09DfUYD+UiIzHCoLRDb1ujJkDzAGra6i1ClRKtQ8hvp5nHIgGa7TVXUeK2H64kO2Hi/jxcAHL9+Ri61XCz9ONPl396BfpT/9u/vSP9KdPVz89W+kc2DMIMoHudZ5HA1n1VxKRZOB1YJIxJt+O9SilOhBfTzeGxQUzLC64dllZRRW7jhSzO7uY1Owidh0p4tMth3ln7SHA6orqGeZLYp3WQ9+ufvjpeEpNsmcQbAB6iUg8cBiYBtxUdwURiQE+Bm42xuyxYy1KqU7A28Ot9grnU4wxZBw7wc4jhbXdSyv35vHx5sO168SGeNe2GBLCfEkI9SEu1EevkLax214wxlSJyK+Ar7FOH33DGLNDRO61vf4q8AQQArxs6+urauyotlJKNUREiAnxJibEm4lJkbXLc4rK+fFwIbuOFLHziBUQi3ZkU/dEyfoHpvt09SMq0PmOPeiVxUopp1FeWc2h/DL255awP6+UnUeK2JlVxIG80tp1fD3d6B3hS+8IP3qG+9Ij3Jde4b50C+iCSwcefE+vLFZKKawzlvp09aNPV78zlpecrGJ3dhGp2cXsyS4mNbuYb3YeZd6G0yc+enu40jPcl57hvvQK96NPVyssOkMLQoNAKeX0fD3dGBobzNDY4DOWHyutIC2nhL05xdb90RJWp515/MHHw5WeEX70CPWhR7gvPcJ86BHmS2yIDx5uLm39RzknGgRKKdWIYB/b3AzxZwZEYVkle3OK2X3UakGk5ZawZn8+H285HRCuLkJssDcJYb70CPehZ5ivLSh8CejSvs5i0iBQSqkWCvB2JyXOmuqzrtKTVezPLWV/XglpOSXsy7XuV+zJpaK6pna9UF8PEkJ9SbC1Hk51OUUFOuY4hAaBUkq1Eh9PNwZEBzAgOuCM5VXVNWQeP1EbDqfC4tudR5lXevo4hKebC3EhPsTbTm9NCPOpDQl7zi2tQaCUUnbm5upCnO3L/VIiznjteGkFabaWw76cEg7ml7Inp5glqUeprD59VmdXfy/uvCieOy9KaP36Wn2LSimlmi3Ix4NhPmdeQQ1WKyLD1oo4dbA6zM/TLjVoECilVDvk5upCfKjVTTShf8TZ33AeOsa5TUoppexGg0AppZycBoFSSjk5DQKllHJyGgRKKeXkNAiUUsrJaRAopZST0yBQSikn1+EmphGRXODQOb49FMhrxXI6K91PZ6f76Ox0HzVPW+2nWGNMWEMvdLggOB8islGnwjw73U9np/vo7HQfNU972E/aNaSUUk5Og0AppZycswXBHEcX0EHofjo73Udnp/uoeRy+n5zqGIFSSqmfcrYWgVJKqXo0CJRSysk5TRCIyEQR2S0iaSLyuKPraQ9EpLuILBWRXSKyQ0QesC0PFpFvRWSv7T7I0bU6moi4isgWEfnC9lz3UT0iEigiC0Qk1fZvaqTupzOJyEO2/2vbReR9EfFqD/vIKYJARFyBl4BJQH9guoj0d2xV7UIV8BtjTD9gBPBL2355HFhijOkFLLE9d3YPALvqPNd99FPPA4uMMX2BgVj7S/eTjYhEAfcDKcaYJMAVmEY72EdOEQTAcCDNGLPfGFMBzAOmOLgmhzPGHDHGbLY9Lsb6jxuFtW/etq32NnCNYypsH0QkGrgCeL3OYt1HdYiIPzAG+BeAMabCGFOA7qf63IAuIuIGeANZtIN95CxBEAVk1HmeaVumbEQkDhgMrAMijDFHwAoLINxxlbULs4DfAjV1luk+OlMCkAu8aetCe11EfND9VMsYcxh4FkgHjgCFxphvaAf7yFmCQBpYpufN2oiIL/AR8KAxpsjR9bQnInIlkGOM2eToWto5N2AI8IoxZjBQihN3AzXE1vc/BYgHugE+IjLTsVVZnCUIMoHudZ5HYzXJnJ6IuGOFwHvGmI9ti4+KSKTt9Uggx1H1tQOjgKtF5CBWl+LFIvIuuo/qywQyjTHrbM8XYAWD7qfTLgUOGGNyjTGVwMfAhbSDfeQsQbAB6CUi8SLigXWA5nMH1+RwIiJYfbq7jDHP1Xnpc+BW2+Nbgc/aurb2whjzO2NMtDEmDuvfzXfGmJnoPjqDMSYbyBCRPrZFlwA70f1UVzowQkS8bf/3LsE6LufwfeQ0VxaLyGSsvl5X4A1jzF8cXJLDichoYCXwI6f7v/8H6zjBfCAG6x/vz4wxxxxSZDsiIuOAR4wxV4pICLqPziAig7AOqHsA+4HbsX5s6n6yEZGngKlYZ+xtAe4EfHHwPnKaIFBKKdUwZ+kaUkop1QgNAqWUcnIaBEop5eQ0CJRSyslpECillJPTIFDKRkSqRWRrnVurXRkrInEisr21tqdUa3JzdAFKtSMnjDGDHF2EUm1NWwRKnYWIHBSR/xOR9bZbT9vyWBFZIiLbbPcxtuURIvKJiPxgu11o25SriMy1jUf/jYh0sa1/v4jstG1nnoP+mMqJaRAodVqXel1DU+u8VmSMGQ68iHWFOrbH/zbGJAPvAbNty2cDy40xA7HG29lhW94LeMkYkwgUANfblj8ODLZt5157/eGUaoxeWayUjYiUGGN8G1h+ELjYGLPfNkhftjEmRETygEhjTKVt+RFjTKiI5ALRxpiTdbYRB3xrm3wEEXkMcDfG/FlEFgElwKfAp8aYEjv/UZU6g7YIlGoe08jjxtZpyMk6j6s5fYzuCqwZ9IYCm2yTlijVZjQIlGqeqXXu19gef481IinADGCV7fES4D6onevYv7GNiogL0N0YsxRr8ptArEHIlGoz+stDqdO6iMjWOs8XGWNOnULqKSLrsH48Tbctux94Q0QexZqd63bb8geAOSLyc6xf/vdhzUjVEFfgXREJwJpA6Z+2KR6VajN6jECps7AdI0gxxuQ5uhal7EG7hpRSyslpi0AppZyctgiUUsrJaRAopZST0yBQSiknp0GglFJOToNAKaWc3P8H+Ir4vrqH594AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(loss_tr_count)),loss_tr_count,label='training loss')\n",
    "plt.plot(range(len(dev_loss_count)),dev_loss_count,label='validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here...\n",
    "\n",
    "**Conclusion:** No overfitting or underfitting. \n",
    "\n",
    "* **Discuss Overfitting:** epochs have not been completed, and learning stopped about 93 epochs, because it stopped before the loss of validation had increased. \n",
    "\n",
    "* **Discuss Underfitting:** Before stopping learning, the loss of validation continues to decrease, and stop learning when the difference in loss reduction of validation is below a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:38.639581Z",
     "start_time": "2020-03-20T23:39:38.590913Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8325\n",
      "Precision: 0.824390243902439\n",
      "Recall: 0.845\n",
      "F1-Score: 0.8345679012345678\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te_count = predict_class(X_te_count,w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:38.679630Z",
     "start_time": "2020-03-20T23:39:38.650914Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'only', 'unfortunately', 'worst', 'script', 'boring', 'why', 'plot', 'any', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "sorted_index = w_count.copy().flatten().argsort()\n",
    "top_neg_ngram = [vocab[i] for i in sorted_index[:10]]\n",
    "print(top_neg_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:38.768621Z",
     "start_time": "2020-03-20T23:39:38.746132Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'well', 'seen', 'fun', 'also', 'many', 'life', 'both', 'world', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "top_pos_ngram = [vocab[i] for i in sorted_index[-10:][::-1]]\n",
    "print(top_pos_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your answer here...\n",
    "\n",
    "**Conclusion:** No, it can't. \n",
    "* **Data distribution:** Although the model can be used in other domain, the learned parameters are not. The data in the training set should be independent and identically distributed. However, after switching domains, the data may never appear in the training set, which means that the data set in this field is distributed in the training set as 0. This can lead to poor performance.\n",
    "* **Vocabulary Corpus:** New words appearing in different domain can also lead to poor performance.\n",
    "\n",
    "According to the existing training models and parameters, the classifier can only extract as important features the same as the training set. Extracting new features can be obtained by retraining the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:52.656205Z",
     "start_time": "2020-03-20T23:39:38.779546Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Training loss: 0.653664| Validation loss: 0.663014\n",
      "Epoch: 1| Training loss: 0.608053| Validation loss: 0.639506\n",
      "Epoch: 2| Training loss: 0.571193| Validation loss: 0.622503\n",
      "Epoch: 3| Training loss: 0.540441| Validation loss: 0.607126\n",
      "Epoch: 4| Training loss: 0.514027| Validation loss: 0.592978\n",
      "Epoch: 5| Training loss: 0.491010| Validation loss: 0.581289\n",
      "Epoch: 6| Training loss: 0.470818| Validation loss: 0.572097\n",
      "Epoch: 7| Training loss: 0.452749| Validation loss: 0.563722\n",
      "Epoch: 8| Training loss: 0.436475| Validation loss: 0.555545\n",
      "Epoch: 9| Training loss: 0.421744| Validation loss: 0.549074\n",
      "Epoch: 10| Training loss: 0.408265| Validation loss: 0.542273\n",
      "Epoch: 11| Training loss: 0.395859| Validation loss: 0.535572\n",
      "Epoch: 12| Training loss: 0.384413| Validation loss: 0.529593\n",
      "Epoch: 13| Training loss: 0.373771| Validation loss: 0.525911\n",
      "Epoch: 14| Training loss: 0.363873| Validation loss: 0.521620\n",
      "Epoch: 15| Training loss: 0.354584| Validation loss: 0.515904\n",
      "Epoch: 16| Training loss: 0.345892| Validation loss: 0.512349\n",
      "Epoch: 17| Training loss: 0.337683| Validation loss: 0.507807\n",
      "Epoch: 18| Training loss: 0.329966| Validation loss: 0.505296\n",
      "Epoch: 19| Training loss: 0.322644| Validation loss: 0.502609\n",
      "Epoch: 20| Training loss: 0.315688| Validation loss: 0.497921\n",
      "Epoch: 21| Training loss: 0.309133| Validation loss: 0.495081\n",
      "Epoch: 22| Training loss: 0.302855| Validation loss: 0.493048\n",
      "Epoch: 23| Training loss: 0.296866| Validation loss: 0.490781\n",
      "Epoch: 24| Training loss: 0.291164| Validation loss: 0.487727\n",
      "Epoch: 25| Training loss: 0.285701| Validation loss: 0.484888\n",
      "Epoch: 26| Training loss: 0.280468| Validation loss: 0.483241\n",
      "Epoch: 27| Training loss: 0.275429| Validation loss: 0.479714\n",
      "Epoch: 28| Training loss: 0.270634| Validation loss: 0.477193\n",
      "Epoch: 29| Training loss: 0.266018| Validation loss: 0.476205\n",
      "Epoch: 30| Training loss: 0.261561| Validation loss: 0.474016\n",
      "Epoch: 31| Training loss: 0.257278| Validation loss: 0.472092\n",
      "Epoch: 32| Training loss: 0.253147| Validation loss: 0.470929\n",
      "Epoch: 33| Training loss: 0.249162| Validation loss: 0.469428\n",
      "Epoch: 34| Training loss: 0.245273| Validation loss: 0.466212\n",
      "Epoch: 35| Training loss: 0.241594| Validation loss: 0.465756\n",
      "Epoch: 36| Training loss: 0.237997| Validation loss: 0.464040\n",
      "Epoch: 37| Training loss: 0.234517| Validation loss: 0.462645\n",
      "Epoch: 38| Training loss: 0.231149| Validation loss: 0.461728\n",
      "Epoch: 39| Training loss: 0.227879| Validation loss: 0.460648\n",
      "Epoch: 40| Training loss: 0.224718| Validation loss: 0.458881\n",
      "Epoch: 41| Training loss: 0.221647| Validation loss: 0.457751\n",
      "Epoch: 42| Training loss: 0.218663| Validation loss: 0.457092\n",
      "Epoch: 43| Training loss: 0.215770| Validation loss: 0.455664\n",
      "Epoch: 44| Training loss: 0.212943| Validation loss: 0.453703\n",
      "Epoch: 45| Training loss: 0.210218| Validation loss: 0.453143\n",
      "Epoch: 46| Training loss: 0.207555| Validation loss: 0.452292\n",
      "Epoch: 47| Training loss: 0.204968| Validation loss: 0.451383\n",
      "Epoch: 48| Training loss: 0.202433| Validation loss: 0.449506\n",
      "Epoch: 49| Training loss: 0.199986| Validation loss: 0.448350\n"
     ]
    }
   ],
   "source": [
    "w_tfidf, trl, devl = SGD(X_tr_tfidf, Y_tr, \n",
    "                         X_dev=X_dev_tfidf, \n",
    "                         Y_dev=Y_dev, \n",
    "                         lr=0.0001, \n",
    "                         alpha=0.0001, \n",
    "                         epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:53.020336Z",
     "start_time": "2020-03-20T23:39:52.665101Z"
    },
    "cell_style": "center",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(trl)),trl,label='training loss')\n",
    "plt.plot(range(len(devl)),devl,label='validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:53.059778Z",
     "start_time": "2020-03-20T23:39:53.029197Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8525\n",
      "Precision: 0.8472906403940886\n",
      "Recall: 0.86\n",
      "F1-Score: 0.8535980148883374\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te = predict_class(X_te_tfidf,w_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top-10 most positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:53.103448Z",
     "start_time": "2020-03-20T23:39:53.065716Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'worst', 'boring', 'supposed', 'why', 'unfortunately', 'stupid', 'harry', 'ridiculous', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "sorted_index = w_tfidf.copy().flatten().argsort()\n",
    "top_neg_ngram = [vocab[i] for i in sorted_index[:10]]\n",
    "print(top_neg_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:39:53.141805Z",
     "start_time": "2020-03-20T23:39:53.109540Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'truman', 'life', 'perfectly', 'hilarious', 'terrific', 'perfect', 'excellent', 'memorable', 'world']\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "top_pos_ngram = [vocab[i] for i in sorted_index[-10:][::-1]]\n",
    "print(top_pos_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your answer here...\n",
    "\n",
    "**Hyperparameters:** Use grid search to find the optimal hyperparameters. This method is simple but time-consuming. Ideally, cross-validation should be used to screen the optimal hyperparameters, but because the training and validation sets have been divided, cross-validation is not very convenient.(Code of grid search is given below)\n",
    "\n",
    "**Relationship:** In simple terms, the smaller the learning rate, the more epochs are needed. But there are some special cases. **Epoch:** If there are too few epochs, there may be insufficient training, resulting in underfitting. If there are too many epochs, the loss may have already converged, and the extra epochs are too wasteful of time.\n",
    "**Learning rate:** Too large a learning rate will not only cause data overflow, but also cause the loss function to fail to converge. Too small a learning rate will consume training time. At the same time, the convex optimization problem should also be considered, otherwise, the loss function may fall into a local minimum.\n",
    "\n",
    "**Regularisation Strength:** Regularization prevents the classifier from overfitting by limiting the number of parameters. The regularization intensity refers to the size of the penalty. The higher the regularization intensity, the more limited the number of parameters. However, when the regularization intensity is too high, the loss function may not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:30.817698Z",
     "start_time": "2020-03-20T23:39:53.147410Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000100, alpha: 0.000100 ---> score:0.830000\n",
      "lr: 0.000100, alpha: 0.000500 ---> score:0.825000\n",
      "lr: 0.000500, alpha: 0.000100 ---> score:0.820000\n",
      "lr: 0.000500, alpha: 0.000500 ---> score:0.830000\n",
      "lr: 0.001000, alpha: 0.000100 ---> score:0.830000\n",
      "lr: 0.001000, alpha: 0.000500 ---> score:0.820000\n",
      "=================Best Hyperparameters=================\n",
      "lr: 0.0001,\talpha: 0.0001,\tscore: 0.83\n"
     ]
    }
   ],
   "source": [
    "def get_F1(lr,alpha):\n",
    "    w_tfidf, trl, devl = SGD(X_tr_tfidf, Y_tr, \n",
    "                         X_dev=X_dev_tfidf, \n",
    "                         Y_dev=Y_dev, \n",
    "                         lr=lr, \n",
    "                         alpha=alpha, \n",
    "                         epochs=100,\n",
    "                         print_progress=False)\n",
    "    preds_dev = predict_class(X_dev_tfidf,w_tfidf)\n",
    "    return accuracy_score(Y_dev,preds_dev)\n",
    "\n",
    "lrs = [0.0001,0.0005,0.001]\n",
    "alphas = [0.0001,0.0005]\n",
    "\n",
    "import itertools\n",
    "record = list()\n",
    "for lr,alpha in itertools.product(lrs,alphas):\n",
    "    score = get_F1(lr, alpha)\n",
    "    print(\"lr: %f, alpha: %f ---> score:%f\"%(lr,alpha,score))\n",
    "    record.append([lr,alpha,score])\n",
    "best = sorted(record,key=lambda l:l[-1],reverse=-1)[0]\n",
    "print(\"=================Best Hyperparameters=================\")\n",
    "print(\"lr: {},\\talpha: {},\\tscore: {}\".format(*best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |0.822   |0.855   |0.838   |\n",
    "| BOW-tfidf  |0.847   |0.860   |0.853   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression \n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:30.912856Z",
     "start_time": "2020-03-20T23:40:30.822511Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading:\n",
      "\ttrain.csv\n",
      "\ttest.csv\n",
      "\tdev.csv\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "Data = FileLoader('data_topic',names=['label','text'])\n",
    "\n",
    "data_tr = Data.train\n",
    "data_te = Data.test\n",
    "data_dev = Data.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:30.963283Z",
     "start_time": "2020-03-20T23:40:30.929808Z"
    },
    "cell_style": "center",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:31.000707Z",
     "start_time": "2020-03-20T23:40:30.978659Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "X_tr_raw, Y_tr = transform(data_tr)\n",
    "X_te_raw, Y_te = transform(data_te)\n",
    "X_dev_raw, Y_dev = transform(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:31.035189Z",
     "start_time": "2020-03-20T23:40:31.008784Z"
    },
    "cell_style": "split",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reuters - Venezuelans turned out early\\\\and in large numbers on Sunday to vote in a historic referendum\\\\that will either remove left-wing President Hugo Chavez from\\\\office or give him a new mandate to govern for the next two\\\\years.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_raw[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:31.061343Z",
     "start_time": "2020-03-20T23:40:31.040110Z"
    },
    "cell_style": "split",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [3],\n",
       "       [3],\n",
       "       [3]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:31.782287Z",
     "start_time": "2020-03-20T23:40:31.075270Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "['reuters', 'said', 'tuesday', 'new', 'wednesday', 'after', 'athens', 'ap', 'monday', 'first', 'their', 'us', 'olympic', 'york', ('new', 'york'), 'two', 'over', 'but', 'oil', 'inc', 'more', 'prices', 'year', 'company', 'world', 'gt', 'lt', 'than', 'aug', 'about', 'had', 'one', 'united', 'out', 'sunday', 'against', 'up', 'quot', 'into', 'fullquote', 'second', 'last', 'stocks', 'team', 'president', 'gold', 'percent', 'iraq', 'three', 'when', 'greece', 'night', ('york', 'reuters'), ('new', 'york', 'reuters'), 'time', 'not', 'no', 'games', 'yesterday', 'home', 'olympics', 'washington', 'states', 'off', ('united', 'states'), 'google', ('oil', 'prices'), 'day', 'public', 'billion', 'record', ('athens', 'reuters'), 'week', 'win', 'all', 'men', 'government', 'won', ('said', 'tuesday'), 'najaf', 'american', 'years', 'officials', 'today', 'city', 'would', 'shares', 'offering', 'people', 'final', 'medal', 'minister', 'afp', 'corp', 'sales', 'million', 'back', 'four', 'investor', 'com']\n",
      "\n",
      "[('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n"
     ]
    }
   ],
   "source": [
    "vocab, df, ngram_counts = get_vocab(X_tr_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:42.980272Z",
     "start_time": "2020-03-20T23:40:31.786996Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "X_tr_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_tr_raw]\n",
    "X_te_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_te_raw]\n",
    "X_dev_ngram = [extract_ngrams(line,ngram_range=(1,3),stop_words=stop_words) for line in X_dev_raw]\n",
    "\n",
    "X_tr_count = vectorise(X_tr_ngram, vocab)\n",
    "X_te_count = vectorise(X_te_ngram, vocab)\n",
    "X_dev_count = vectorise(X_dev_ngram, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:42.999640Z",
     "start_time": "2020-03-20T23:40:42.984007Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \n",
    "    # fill in your code...\n",
    "    z = z.copy()\n",
    "    #z -= np.max(z)\n",
    "    smax = np.exp(z) / np.sum(np.exp(z),axis=1,keepdims=True)\n",
    "    \n",
    "    return smax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.029102Z",
     "start_time": "2020-03-20T23:40:43.003540Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    preds_proba = X.dot(weights.T)\n",
    "    \n",
    "    assert preds_proba.shape == (X.shape[0],weights.shape[0])\n",
    "    return softmax(preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.056556Z",
     "start_time": "2020-03-20T23:40:43.034946Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    # fill in your code...\n",
    "    preds_class = np.argmax(predict_proba(X, weights),axis=1)+1\n",
    "    \n",
    "    assert preds_class.shape == (X.shape[0],)\n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example and expected functionality of the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.078388Z",
     "start_time": "2020-03-20T23:40:43.065089Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.1,0.2],[0.2,0.1],[0.1,-0.2]])\n",
    "w = np.array([[2,-5],[-5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.107490Z",
     "start_time": "2020-03-20T23:40:43.085169Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33181223, 0.66818777],\n",
       "       [0.66818777, 0.33181223],\n",
       "       [0.89090318, 0.10909682]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.141582Z",
     "start_time": "2020-03-20T23:40:43.118907Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.171628Z",
     "start_time": "2020-03-20T23:40:43.149201Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=5, alpha=0.00001):\n",
    "    assert Y.shape[0] == X.shape[0]\n",
    "    eps = np.finfo(np.float64).eps\n",
    "    preds_proba = predict_proba(X, weights)\n",
    "    Y = np.eye(weights.shape[0],weights.shape[0])[Y-1].reshape(X.shape[0],weights.shape[0])\n",
    "    l1 = -np.sum(Y*np.log(preds_proba+eps),1) \n",
    "    l2 = alpha*np.sum(np.multiply(weights,weights))\n",
    "    l=np.mean(l1)+l2/X.shape[0]\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.195179Z",
     "start_time": "2020-03-20T23:40:43.179029Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, weights, alpha=0.00001):\n",
    "    #m = len(X)\n",
    "    Y_c = Y.copy()\n",
    "    Y = np.eye(weights.shape[0],weights.shape[0])[Y-1].reshape(X.shape[0],weights.shape[0])\n",
    "    h = predict_proba(X, weights)\n",
    "    #raise\n",
    "    dw = -np.dot(np.sum(Y-Y*h,axis=1),X) + 2*alpha*weights[(Y_c-1).item()]\n",
    "    return dw#/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:40:43.239331Z",
     "start_time": "2020-03-20T23:40:43.204672Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], num_classes=5, lr=0.01, alpha=0.00001, epochs=5, tolerance=0.001, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    # fill in your code...\n",
    "    # Stage 1: Init weights\n",
    "    weights = np.zeros((num_classes,X_tr.shape[1]))\n",
    "    \n",
    "    # Stage 2: Init stochastic value\n",
    "    idx_list = np.array(range(X_tr.shape[0]))\n",
    "    \n",
    "    # Stage 3: Training\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(idx_list) # disorder dataset\n",
    "        for i in idx_list:\n",
    "            X_tr_i, Y_tr_i = X_tr[i].reshape(1,-1),Y_tr[i].reshape(1,-1) # get a single data pair\n",
    "            dw = gradient(X_tr_i, Y_tr_i, weights, alpha) # gradient\n",
    "            weights[(Y_tr_i-1).item()] -= lr*dw # update       \n",
    "        loss_dev = categorical_loss(X_dev_count, Y_dev, weights, alpha)\n",
    "        loss_tr = categorical_loss(X_tr_count, Y_tr, weights, alpha)\n",
    "\n",
    "        # Add history\n",
    "        if epoch != 0:\n",
    "            training_loss_history.append(loss_tr)\n",
    "            validation_loss_history.append(loss_dev)\n",
    "        if print_progress == True: \n",
    "            print(\"Epoch: %d| Training loss: %f| Validation loss: %f\"%(epoch,loss_tr,loss_dev))\n",
    "        if epoch >1 and abs(validation_loss_history[-2]-validation_loss_history[-1]) <= tolerance:\n",
    "            break\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you MLR following the same steps as in Task 1 for both Count and tfidf features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:41:54.315325Z",
     "start_time": "2020-03-20T23:40:43.244815Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Training loss: 1.078241| Validation loss: 1.088258\n",
      "Epoch: 1| Training loss: 1.059461| Validation loss: 1.078389\n",
      "Epoch: 2| Training loss: 1.042067| Validation loss: 1.068953\n",
      "Epoch: 3| Training loss: 1.025873| Validation loss: 1.059905\n",
      "Epoch: 4| Training loss: 1.010689| Validation loss: 1.051192\n",
      "Epoch: 5| Training loss: 0.996384| Validation loss: 1.042785\n",
      "Epoch: 6| Training loss: 0.982836| Validation loss: 1.034648\n",
      "Epoch: 7| Training loss: 0.969951| Validation loss: 1.026755\n",
      "Epoch: 8| Training loss: 0.957657| Validation loss: 1.019088\n",
      "Epoch: 9| Training loss: 0.945893| Validation loss: 1.011627\n",
      "Epoch: 10| Training loss: 0.934609| Validation loss: 1.004358\n",
      "Epoch: 11| Training loss: 0.923767| Validation loss: 0.997269\n",
      "Epoch: 12| Training loss: 0.913330| Validation loss: 0.990351\n",
      "Epoch: 13| Training loss: 0.903270| Validation loss: 0.983593\n",
      "Epoch: 14| Training loss: 0.893560| Validation loss: 0.976989\n",
      "Epoch: 15| Training loss: 0.884180| Validation loss: 0.970530\n",
      "Epoch: 16| Training loss: 0.875107| Validation loss: 0.964212\n",
      "Epoch: 17| Training loss: 0.866326| Validation loss: 0.958029\n",
      "Epoch: 18| Training loss: 0.857819| Validation loss: 0.951975\n",
      "Epoch: 19| Training loss: 0.849572| Validation loss: 0.946045\n",
      "Epoch: 20| Training loss: 0.841574| Validation loss: 0.940236\n",
      "Epoch: 21| Training loss: 0.833809| Validation loss: 0.934544\n",
      "Epoch: 22| Training loss: 0.826268| Validation loss: 0.928964\n",
      "Epoch: 23| Training loss: 0.818939| Validation loss: 0.923493\n",
      "Epoch: 24| Training loss: 0.811812| Validation loss: 0.918127\n",
      "Epoch: 25| Training loss: 0.804879| Validation loss: 0.912864\n",
      "Epoch: 26| Training loss: 0.798131| Validation loss: 0.907701\n",
      "Epoch: 27| Training loss: 0.791562| Validation loss: 0.902634\n",
      "Epoch: 28| Training loss: 0.785162| Validation loss: 0.897662\n",
      "Epoch: 29| Training loss: 0.778926| Validation loss: 0.892781\n",
      "Epoch: 30| Training loss: 0.772845| Validation loss: 0.887988\n",
      "Epoch: 31| Training loss: 0.766915| Validation loss: 0.883283\n",
      "Epoch: 32| Training loss: 0.761129| Validation loss: 0.878661\n",
      "Epoch: 33| Training loss: 0.755482| Validation loss: 0.874122\n",
      "Epoch: 34| Training loss: 0.749968| Validation loss: 0.869662\n",
      "Epoch: 35| Training loss: 0.744582| Validation loss: 0.865280\n",
      "Epoch: 36| Training loss: 0.739319| Validation loss: 0.860974\n",
      "Epoch: 37| Training loss: 0.734175| Validation loss: 0.856741\n",
      "Epoch: 38| Training loss: 0.729146| Validation loss: 0.852580\n",
      "Epoch: 39| Training loss: 0.724228| Validation loss: 0.848491\n",
      "Epoch: 40| Training loss: 0.719416| Validation loss: 0.844469\n",
      "Epoch: 41| Training loss: 0.714707| Validation loss: 0.840514\n",
      "Epoch: 42| Training loss: 0.710098| Validation loss: 0.836624\n",
      "Epoch: 43| Training loss: 0.705584| Validation loss: 0.832798\n",
      "Epoch: 44| Training loss: 0.701163| Validation loss: 0.829033\n",
      "Epoch: 45| Training loss: 0.696833| Validation loss: 0.825329\n",
      "Epoch: 46| Training loss: 0.692588| Validation loss: 0.821685\n",
      "Epoch: 47| Training loss: 0.688428| Validation loss: 0.818097\n",
      "Epoch: 48| Training loss: 0.684350| Validation loss: 0.814567\n",
      "Epoch: 49| Training loss: 0.680349| Validation loss: 0.811091\n",
      "Epoch: 50| Training loss: 0.676426| Validation loss: 0.807669\n",
      "Epoch: 51| Training loss: 0.672576| Validation loss: 0.804300\n",
      "Epoch: 52| Training loss: 0.668798| Validation loss: 0.800981\n",
      "Epoch: 53| Training loss: 0.665088| Validation loss: 0.797713\n",
      "Epoch: 54| Training loss: 0.661447| Validation loss: 0.794494\n",
      "Epoch: 55| Training loss: 0.657872| Validation loss: 0.791323\n",
      "Epoch: 56| Training loss: 0.654359| Validation loss: 0.788199\n",
      "Epoch: 57| Training loss: 0.650908| Validation loss: 0.785120\n",
      "Epoch: 58| Training loss: 0.647518| Validation loss: 0.782086\n",
      "Epoch: 59| Training loss: 0.644185| Validation loss: 0.779096\n",
      "Epoch: 60| Training loss: 0.640909| Validation loss: 0.776149\n",
      "Epoch: 61| Training loss: 0.637688| Validation loss: 0.773244\n",
      "Epoch: 62| Training loss: 0.634520| Validation loss: 0.770379\n",
      "Epoch: 63| Training loss: 0.631405| Validation loss: 0.767555\n",
      "Epoch: 64| Training loss: 0.628340| Validation loss: 0.764771\n",
      "Epoch: 65| Training loss: 0.625324| Validation loss: 0.762024\n",
      "Epoch: 66| Training loss: 0.622356| Validation loss: 0.759316\n",
      "Epoch: 67| Training loss: 0.619436| Validation loss: 0.756644\n",
      "Epoch: 68| Training loss: 0.616560| Validation loss: 0.754008\n",
      "Epoch: 69| Training loss: 0.613729| Validation loss: 0.751408\n",
      "Epoch: 70| Training loss: 0.610941| Validation loss: 0.748842\n",
      "Epoch: 71| Training loss: 0.608195| Validation loss: 0.746310\n",
      "Epoch: 72| Training loss: 0.605490| Validation loss: 0.743811\n",
      "Epoch: 73| Training loss: 0.602825| Validation loss: 0.741345\n",
      "Epoch: 74| Training loss: 0.600199| Validation loss: 0.738911\n",
      "Epoch: 75| Training loss: 0.597611| Validation loss: 0.736508\n",
      "Epoch: 76| Training loss: 0.595060| Validation loss: 0.734136\n",
      "Epoch: 77| Training loss: 0.592546| Validation loss: 0.731793\n",
      "Epoch: 78| Training loss: 0.590067| Validation loss: 0.729480\n",
      "Epoch: 79| Training loss: 0.587622| Validation loss: 0.727196\n",
      "Epoch: 80| Training loss: 0.585212| Validation loss: 0.724941\n",
      "Epoch: 81| Training loss: 0.582834| Validation loss: 0.722713\n",
      "Epoch: 82| Training loss: 0.580489| Validation loss: 0.720512\n",
      "Epoch: 83| Training loss: 0.578175| Validation loss: 0.718338\n",
      "Epoch: 84| Training loss: 0.575891| Validation loss: 0.716190\n",
      "Epoch: 85| Training loss: 0.573638| Validation loss: 0.714068\n",
      "Epoch: 86| Training loss: 0.571415| Validation loss: 0.711971\n",
      "Epoch: 87| Training loss: 0.569220| Validation loss: 0.709899\n",
      "Epoch: 88| Training loss: 0.567054| Validation loss: 0.707851\n",
      "Epoch: 89| Training loss: 0.564914| Validation loss: 0.705827\n",
      "Epoch: 90| Training loss: 0.562802| Validation loss: 0.703826\n",
      "Epoch: 91| Training loss: 0.560716| Validation loss: 0.701849\n",
      "Epoch: 92| Training loss: 0.558657| Validation loss: 0.699894\n",
      "Epoch: 93| Training loss: 0.556622| Validation loss: 0.697961\n",
      "Epoch: 94| Training loss: 0.554613| Validation loss: 0.696050\n",
      "Epoch: 95| Training loss: 0.552627| Validation loss: 0.694161\n",
      "Epoch: 96| Training loss: 0.550665| Validation loss: 0.692292\n",
      "Epoch: 97| Training loss: 0.548727| Validation loss: 0.690444\n",
      "Epoch: 98| Training loss: 0.546811| Validation loss: 0.688617\n",
      "Epoch: 99| Training loss: 0.544918| Validation loss: 0.686809\n",
      "Epoch: 100| Training loss: 0.543047| Validation loss: 0.685022\n",
      "Epoch: 101| Training loss: 0.541197| Validation loss: 0.683253\n",
      "Epoch: 102| Training loss: 0.539369| Validation loss: 0.681503\n",
      "Epoch: 103| Training loss: 0.537561| Validation loss: 0.679772\n",
      "Epoch: 104| Training loss: 0.535773| Validation loss: 0.678059\n",
      "Epoch: 105| Training loss: 0.534005| Validation loss: 0.676365\n",
      "Epoch: 106| Training loss: 0.532257| Validation loss: 0.674687\n",
      "Epoch: 107| Training loss: 0.530528| Validation loss: 0.673028\n",
      "Epoch: 108| Training loss: 0.528818| Validation loss: 0.671385\n",
      "Epoch: 109| Training loss: 0.527126| Validation loss: 0.669759\n",
      "Epoch: 110| Training loss: 0.525452| Validation loss: 0.668150\n",
      "Epoch: 111| Training loss: 0.523796| Validation loss: 0.666557\n",
      "Epoch: 112| Training loss: 0.522157| Validation loss: 0.664981\n",
      "Epoch: 113| Training loss: 0.520536| Validation loss: 0.663420\n",
      "Epoch: 114| Training loss: 0.518931| Validation loss: 0.661874\n",
      "Epoch: 115| Training loss: 0.517343| Validation loss: 0.660344\n",
      "Epoch: 116| Training loss: 0.515772| Validation loss: 0.658829\n",
      "Epoch: 117| Training loss: 0.514216| Validation loss: 0.657329\n",
      "Epoch: 118| Training loss: 0.512676| Validation loss: 0.655843\n",
      "Epoch: 119| Training loss: 0.511151| Validation loss: 0.654372\n",
      "Epoch: 120| Training loss: 0.509642| Validation loss: 0.652915\n",
      "Epoch: 121| Training loss: 0.508147| Validation loss: 0.651471\n",
      "Epoch: 122| Training loss: 0.506667| Validation loss: 0.650042\n",
      "Epoch: 123| Training loss: 0.505201| Validation loss: 0.648626\n",
      "Epoch: 124| Training loss: 0.503750| Validation loss: 0.647223\n",
      "Epoch: 125| Training loss: 0.502312| Validation loss: 0.645834\n",
      "Epoch: 126| Training loss: 0.500889| Validation loss: 0.644457\n",
      "Epoch: 127| Training loss: 0.499478| Validation loss: 0.643093\n",
      "Epoch: 128| Training loss: 0.498081| Validation loss: 0.641742\n",
      "Epoch: 129| Training loss: 0.496697| Validation loss: 0.640403\n",
      "Epoch: 130| Training loss: 0.495326| Validation loss: 0.639076\n",
      "Epoch: 131| Training loss: 0.493968| Validation loss: 0.637761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132| Training loss: 0.492622| Validation loss: 0.636458\n",
      "Epoch: 133| Training loss: 0.491288| Validation loss: 0.635167\n",
      "Epoch: 134| Training loss: 0.489966| Validation loss: 0.633887\n",
      "Epoch: 135| Training loss: 0.488656| Validation loss: 0.632618\n",
      "Epoch: 136| Training loss: 0.487357| Validation loss: 0.631361\n",
      "Epoch: 137| Training loss: 0.486071| Validation loss: 0.630114\n",
      "Epoch: 138| Training loss: 0.484795| Validation loss: 0.628879\n",
      "Epoch: 139| Training loss: 0.483531| Validation loss: 0.627654\n",
      "Epoch: 140| Training loss: 0.482278| Validation loss: 0.626440\n",
      "Epoch: 141| Training loss: 0.481035| Validation loss: 0.625236\n",
      "Epoch: 142| Training loss: 0.479803| Validation loss: 0.624042\n",
      "Epoch: 143| Training loss: 0.478581| Validation loss: 0.622858\n",
      "Epoch: 144| Training loss: 0.477370| Validation loss: 0.621684\n",
      "Epoch: 145| Training loss: 0.476169| Validation loss: 0.620520\n",
      "Epoch: 146| Training loss: 0.474978| Validation loss: 0.619366\n",
      "Epoch: 147| Training loss: 0.473797| Validation loss: 0.618221\n",
      "Epoch: 148| Training loss: 0.472626| Validation loss: 0.617086\n",
      "Epoch: 149| Training loss: 0.471465| Validation loss: 0.615960\n",
      "Epoch: 150| Training loss: 0.470313| Validation loss: 0.614844\n",
      "Epoch: 151| Training loss: 0.469170| Validation loss: 0.613737\n",
      "Epoch: 152| Training loss: 0.468037| Validation loss: 0.612638\n",
      "Epoch: 153| Training loss: 0.466912| Validation loss: 0.611549\n",
      "Epoch: 154| Training loss: 0.465797| Validation loss: 0.610468\n",
      "Epoch: 155| Training loss: 0.464690| Validation loss: 0.609396\n",
      "Epoch: 156| Training loss: 0.463592| Validation loss: 0.608332\n",
      "Epoch: 157| Training loss: 0.462503| Validation loss: 0.607277\n",
      "Epoch: 158| Training loss: 0.461422| Validation loss: 0.606230\n",
      "Epoch: 159| Training loss: 0.460350| Validation loss: 0.605192\n",
      "Epoch: 160| Training loss: 0.459286| Validation loss: 0.604161\n",
      "Epoch: 161| Training loss: 0.458230| Validation loss: 0.603138\n",
      "Epoch: 162| Training loss: 0.457182| Validation loss: 0.602124\n",
      "Epoch: 163| Training loss: 0.456142| Validation loss: 0.601117\n",
      "Epoch: 164| Training loss: 0.455110| Validation loss: 0.600118\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, Y_tr, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=Y_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:41:54.711834Z",
     "start_time": "2020-03-20T23:41:54.322961Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fill in your code...\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(loss_tr_count)),loss_tr_count,label='training loss')\n",
    "plt.plot(range(len(dev_loss_count)),dev_loss_count,label='validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:41:54.803713Z",
     "start_time": "2020-03-20T23:41:54.721180Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8366666666666667\n",
      "Precision: 0.840344708396179\n",
      "Recall: 0.8366666666666666\n",
      "F1-Score: 0.8359703962026542\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te = predict_class(X_te_count,w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te.flatten(),preds_te))\n",
    "print('Precision:', precision_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te.flatten(),preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:41:54.840577Z",
     "start_time": "2020-03-20T23:41:54.807036Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'reuters', 'tuesday', 'ap', 'after', 'monday', 'new', 'wednesday', 'their', 'president']\n",
      "['athens', 'ap', 'reuters', 'olympic', 'tuesday', 'first', 'after', 'team', 'two', 'wednesday']\n",
      "['reuters', 'said', 'new', 'tuesday', 'company', 'oil', 'wednesday', 'prices', 'inc', 'after']\n"
     ]
    }
   ],
   "source": [
    "sorted_index = w_count.copy().argsort()[:,-10:][:,::-1]\n",
    "for class_i in range(len(sorted_index)):\n",
    "    top_neg_ngram = [vocab[i] for i in sorted_index[class_i]]\n",
    "    print(top_neg_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:19.856538Z",
     "start_time": "2020-02-15T14:16:19.852547Z"
    }
   },
   "source": [
    "Explain here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:45:52.263160Z",
     "start_time": "2020-03-20T23:41:54.851974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000100, alpha: 0.000100 ---> score:0.880000\n",
      "lr: 0.000100, alpha: 0.100000 ---> score:0.860000\n",
      "lr: 0.000100, alpha: 10.000000 ---> score:0.833333\n",
      "lr: 0.000500, alpha: 0.000100 ---> score:0.886667\n",
      "lr: 0.000500, alpha: 0.100000 ---> score:0.873333\n",
      "lr: 0.000500, alpha: 10.000000 ---> score:0.800000\n",
      "lr: 0.001000, alpha: 0.000100 ---> score:0.886667\n",
      "lr: 0.001000, alpha: 0.100000 ---> score:0.880000\n",
      "lr: 0.001000, alpha: 10.000000 ---> score:0.813333\n",
      "=================Best Hyperparameters=================\n",
      "lr: 0.0005,\talpha: 0.0001,\tscore: 0.8866666666666667\n",
      "----------------set dataset evaluation----------------\n",
      "Accuracy: 0.8455555555555555\n",
      "Precision: 0.8503930523024751\n",
      "Recall: 0.8455555555555555\n",
      "F1-Score: 0.8450029165279874\n"
     ]
    }
   ],
   "source": [
    "def get_F1(lr,alpha):\n",
    "    w, trl, devl = SGD(X_tr_count, Y_tr, \n",
    "                         X_dev=X_dev_count, \n",
    "                         Y_dev=Y_dev, \n",
    "                         num_classes=3,\n",
    "                         lr=lr, \n",
    "                         alpha=alpha, \n",
    "                         epochs=200,\n",
    "                         print_progress=False)\n",
    "    preds_dev = predict_class(X_dev_count,w)\n",
    "    return accuracy_score(Y_dev,preds_dev)\n",
    "\n",
    "lrs = [0.0001,0.0005,0.001]\n",
    "alphas = [0.0001,0.1,10]\n",
    "\n",
    "import itertools\n",
    "record = list()\n",
    "for lr,alpha in itertools.product(lrs,alphas):\n",
    "    score = get_F1(lr, alpha)\n",
    "    print(\"lr: %f, alpha: %f ---> score:%f\"%(lr,alpha,score))\n",
    "    record.append([lr,alpha,score])\n",
    "best = sorted(record,key=lambda l:l[-1],reverse=-1)[0]\n",
    "print(\"=================Best Hyperparameters=================\")\n",
    "print(\"lr: {},\\talpha: {},\\tscore: {}\".format(*best))\n",
    "print(\"----------------set dataset evaluation----------------\")\n",
    "lr,alpha=best[:-1]\n",
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, Y_tr, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=Y_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=lr, \n",
    "                                             alpha=alpha, \n",
    "                                             epochs=200,\n",
    "                                             print_progress=False)\n",
    "\n",
    "preds_te = predict_class(X_te_count,w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te.flatten(),preds_te))\n",
    "print('Precision:', precision_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te.flatten(),preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate BOW-tfidf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:45:52.287230Z",
     "start_time": "2020-03-20T23:45:52.268651Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58018188, 0.74472749, 0.76426119, ..., 2.77815125, 2.77815125,\n",
       "       2.77815125])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = get_idfs(vocab,len(X_tr_raw))\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:45:52.390979Z",
     "start_time": "2020-03-20T23:45:52.296300Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_tfidf = X_tr_count*idfs\n",
    "X_te_tfidf = X_te_count*idfs\n",
    "X_dev_tfidf = X_dev_count*idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:46:49.664750Z",
     "start_time": "2020-03-20T23:45:52.397513Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Training loss: 1.068237| Validation loss: 1.082842\n",
      "Epoch: 1| Training loss: 1.041725| Validation loss: 1.068350\n",
      "Epoch: 2| Training loss: 1.018067| Validation loss: 1.054869\n",
      "Epoch: 3| Training loss: 0.996636| Validation loss: 1.042240\n",
      "Epoch: 4| Training loss: 0.976935| Validation loss: 1.030316\n",
      "Epoch: 5| Training loss: 0.958684| Validation loss: 1.019014\n",
      "Epoch: 6| Training loss: 0.941681| Validation loss: 1.008263\n",
      "Epoch: 7| Training loss: 0.925759| Validation loss: 0.998016\n",
      "Epoch: 8| Training loss: 0.910801| Validation loss: 0.988232\n",
      "Epoch: 9| Training loss: 0.896711| Validation loss: 0.978873\n",
      "Epoch: 10| Training loss: 0.883403| Validation loss: 0.969905\n",
      "Epoch: 11| Training loss: 0.870800| Validation loss: 0.961305\n",
      "Epoch: 12| Training loss: 0.858846| Validation loss: 0.953043\n",
      "Epoch: 13| Training loss: 0.847488| Validation loss: 0.945103\n",
      "Epoch: 14| Training loss: 0.836673| Validation loss: 0.937460\n",
      "Epoch: 15| Training loss: 0.826356| Validation loss: 0.930096\n",
      "Epoch: 16| Training loss: 0.816501| Validation loss: 0.922993\n",
      "Epoch: 17| Training loss: 0.807078| Validation loss: 0.916138\n",
      "Epoch: 18| Training loss: 0.798051| Validation loss: 0.909516\n",
      "Epoch: 19| Training loss: 0.789394| Validation loss: 0.903112\n",
      "Epoch: 20| Training loss: 0.781084| Validation loss: 0.896917\n",
      "Epoch: 21| Training loss: 0.773094| Validation loss: 0.890918\n",
      "Epoch: 22| Training loss: 0.765408| Validation loss: 0.885108\n",
      "Epoch: 23| Training loss: 0.758003| Validation loss: 0.879472\n",
      "Epoch: 24| Training loss: 0.750867| Validation loss: 0.874006\n",
      "Epoch: 25| Training loss: 0.743979| Validation loss: 0.868700\n",
      "Epoch: 26| Training loss: 0.737324| Validation loss: 0.863544\n",
      "Epoch: 27| Training loss: 0.730893| Validation loss: 0.858533\n",
      "Epoch: 28| Training loss: 0.724673| Validation loss: 0.853660\n",
      "Epoch: 29| Training loss: 0.718648| Validation loss: 0.848919\n",
      "Epoch: 30| Training loss: 0.712809| Validation loss: 0.844302\n",
      "Epoch: 31| Training loss: 0.707149| Validation loss: 0.839806\n",
      "Epoch: 32| Training loss: 0.701657| Validation loss: 0.835423\n",
      "Epoch: 33| Training loss: 0.696325| Validation loss: 0.831152\n",
      "Epoch: 34| Training loss: 0.691145| Validation loss: 0.826986\n",
      "Epoch: 35| Training loss: 0.686111| Validation loss: 0.822921\n",
      "Epoch: 36| Training loss: 0.681213| Validation loss: 0.818953\n",
      "Epoch: 37| Training loss: 0.676447| Validation loss: 0.815077\n",
      "Epoch: 38| Training loss: 0.671807| Validation loss: 0.811292\n",
      "Epoch: 39| Training loss: 0.667285| Validation loss: 0.807592\n",
      "Epoch: 40| Training loss: 0.662879| Validation loss: 0.803974\n",
      "Epoch: 41| Training loss: 0.658582| Validation loss: 0.800435\n",
      "Epoch: 42| Training loss: 0.654389| Validation loss: 0.796973\n",
      "Epoch: 43| Training loss: 0.650299| Validation loss: 0.793586\n",
      "Epoch: 44| Training loss: 0.646304| Validation loss: 0.790270\n",
      "Epoch: 45| Training loss: 0.642402| Validation loss: 0.787022\n",
      "Epoch: 46| Training loss: 0.638590| Validation loss: 0.783840\n",
      "Epoch: 47| Training loss: 0.634861| Validation loss: 0.780721\n",
      "Epoch: 48| Training loss: 0.631215| Validation loss: 0.777665\n",
      "Epoch: 49| Training loss: 0.627649| Validation loss: 0.774668\n",
      "Epoch: 50| Training loss: 0.624158| Validation loss: 0.771728\n",
      "Epoch: 51| Training loss: 0.620742| Validation loss: 0.768846\n",
      "Epoch: 52| Training loss: 0.617395| Validation loss: 0.766016\n",
      "Epoch: 53| Training loss: 0.614117| Validation loss: 0.763239\n",
      "Epoch: 54| Training loss: 0.610904| Validation loss: 0.760512\n",
      "Epoch: 55| Training loss: 0.607755| Validation loss: 0.757835\n",
      "Epoch: 56| Training loss: 0.604667| Validation loss: 0.755204\n",
      "Epoch: 57| Training loss: 0.601639| Validation loss: 0.752621\n",
      "Epoch: 58| Training loss: 0.598669| Validation loss: 0.750083\n",
      "Epoch: 59| Training loss: 0.595753| Validation loss: 0.747588\n",
      "Epoch: 60| Training loss: 0.592891| Validation loss: 0.745134\n",
      "Epoch: 61| Training loss: 0.590080| Validation loss: 0.742721\n",
      "Epoch: 62| Training loss: 0.587320| Validation loss: 0.740348\n",
      "Epoch: 63| Training loss: 0.584609| Validation loss: 0.738013\n",
      "Epoch: 64| Training loss: 0.581946| Validation loss: 0.735717\n",
      "Epoch: 65| Training loss: 0.579328| Validation loss: 0.733457\n",
      "Epoch: 66| Training loss: 0.576754| Validation loss: 0.731232\n",
      "Epoch: 67| Training loss: 0.574224| Validation loss: 0.729042\n",
      "Epoch: 68| Training loss: 0.571735| Validation loss: 0.726885\n",
      "Epoch: 69| Training loss: 0.569288| Validation loss: 0.724762\n",
      "Epoch: 70| Training loss: 0.566880| Validation loss: 0.722670\n",
      "Epoch: 71| Training loss: 0.564512| Validation loss: 0.720611\n",
      "Epoch: 72| Training loss: 0.562180| Validation loss: 0.718581\n",
      "Epoch: 73| Training loss: 0.559884| Validation loss: 0.716580\n",
      "Epoch: 74| Training loss: 0.557624| Validation loss: 0.714609\n",
      "Epoch: 75| Training loss: 0.555399| Validation loss: 0.712665\n",
      "Epoch: 76| Training loss: 0.553206| Validation loss: 0.710749\n",
      "Epoch: 77| Training loss: 0.551047| Validation loss: 0.708860\n",
      "Epoch: 78| Training loss: 0.548919| Validation loss: 0.706997\n",
      "Epoch: 79| Training loss: 0.546823| Validation loss: 0.705159\n",
      "Epoch: 80| Training loss: 0.544757| Validation loss: 0.703347\n",
      "Epoch: 81| Training loss: 0.542721| Validation loss: 0.701560\n",
      "Epoch: 82| Training loss: 0.540712| Validation loss: 0.699795\n",
      "Epoch: 83| Training loss: 0.538733| Validation loss: 0.698053\n",
      "Epoch: 84| Training loss: 0.536780| Validation loss: 0.696335\n",
      "Epoch: 85| Training loss: 0.534856| Validation loss: 0.694639\n",
      "Epoch: 86| Training loss: 0.532956| Validation loss: 0.692964\n",
      "Epoch: 87| Training loss: 0.531082| Validation loss: 0.691311\n",
      "Epoch: 88| Training loss: 0.529233| Validation loss: 0.689679\n",
      "Epoch: 89| Training loss: 0.527408| Validation loss: 0.688066\n",
      "Epoch: 90| Training loss: 0.525607| Validation loss: 0.686475\n",
      "Epoch: 91| Training loss: 0.523830| Validation loss: 0.684902\n",
      "Epoch: 92| Training loss: 0.522075| Validation loss: 0.683348\n",
      "Epoch: 93| Training loss: 0.520342| Validation loss: 0.681813\n",
      "Epoch: 94| Training loss: 0.518630| Validation loss: 0.680296\n",
      "Epoch: 95| Training loss: 0.516941| Validation loss: 0.678797\n",
      "Epoch: 96| Training loss: 0.515272| Validation loss: 0.677316\n",
      "Epoch: 97| Training loss: 0.513623| Validation loss: 0.675852\n",
      "Epoch: 98| Training loss: 0.511994| Validation loss: 0.674405\n",
      "Epoch: 99| Training loss: 0.510385| Validation loss: 0.672974\n",
      "Epoch: 100| Training loss: 0.508795| Validation loss: 0.671560\n",
      "Epoch: 101| Training loss: 0.507223| Validation loss: 0.670161\n",
      "Epoch: 102| Training loss: 0.505670| Validation loss: 0.668778\n",
      "Epoch: 103| Training loss: 0.504134| Validation loss: 0.667410\n",
      "Epoch: 104| Training loss: 0.502617| Validation loss: 0.666058\n",
      "Epoch: 105| Training loss: 0.501117| Validation loss: 0.664720\n",
      "Epoch: 106| Training loss: 0.499634| Validation loss: 0.663398\n",
      "Epoch: 107| Training loss: 0.498167| Validation loss: 0.662089\n",
      "Epoch: 108| Training loss: 0.496717| Validation loss: 0.660793\n",
      "Epoch: 109| Training loss: 0.495283| Validation loss: 0.659511\n",
      "Epoch: 110| Training loss: 0.493864| Validation loss: 0.658243\n",
      "Epoch: 111| Training loss: 0.492461| Validation loss: 0.656989\n",
      "Epoch: 112| Training loss: 0.491072| Validation loss: 0.655747\n",
      "Epoch: 113| Training loss: 0.489698| Validation loss: 0.654518\n",
      "Epoch: 114| Training loss: 0.488339| Validation loss: 0.653301\n",
      "Epoch: 115| Training loss: 0.486995| Validation loss: 0.652097\n",
      "Epoch: 116| Training loss: 0.485664| Validation loss: 0.650905\n",
      "Epoch: 117| Training loss: 0.484347| Validation loss: 0.649725\n",
      "Epoch: 118| Training loss: 0.483044| Validation loss: 0.648557\n",
      "Epoch: 119| Training loss: 0.481753| Validation loss: 0.647400\n",
      "Epoch: 120| Training loss: 0.480477| Validation loss: 0.646255\n",
      "Epoch: 121| Training loss: 0.479212| Validation loss: 0.645121\n",
      "Epoch: 122| Training loss: 0.477960| Validation loss: 0.643997\n",
      "Epoch: 123| Training loss: 0.476721| Validation loss: 0.642884\n",
      "Epoch: 124| Training loss: 0.475493| Validation loss: 0.641782\n",
      "Epoch: 125| Training loss: 0.474278| Validation loss: 0.640689\n",
      "Epoch: 126| Training loss: 0.473074| Validation loss: 0.639608\n",
      "Epoch: 127| Training loss: 0.471882| Validation loss: 0.638536\n",
      "Epoch: 128| Training loss: 0.470701| Validation loss: 0.637475\n",
      "Epoch: 129| Training loss: 0.469531| Validation loss: 0.636422\n",
      "Epoch: 130| Training loss: 0.468371| Validation loss: 0.635379\n",
      "Epoch: 131| Training loss: 0.467224| Validation loss: 0.634346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132| Training loss: 0.466086| Validation loss: 0.633322\n",
      "Epoch: 133| Training loss: 0.464959| Validation loss: 0.632307\n",
      "Epoch: 134| Training loss: 0.463842| Validation loss: 0.631301\n",
      "Epoch: 135| Training loss: 0.462736| Validation loss: 0.630305\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_tfidf, Y_tr, \n",
    "                                             X_dev=X_dev_tfidf, \n",
    "                                             Y_dev=Y_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:46:49.760755Z",
     "start_time": "2020-03-20T23:46:49.669988Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8777777777777778\n",
      "Precision: 0.8806469941082025\n",
      "Recall: 0.8777777777777779\n",
      "F1-Score: 0.877232813452245\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te = predict_class(X_te_tfidf,w_count).reshape(-1,1)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:46:49.789506Z",
     "start_time": "2020-03-20T23:46:49.767070Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'tuesday', 'afp', 'ap', 'monday', 'president', 'their', 'after', 'new', 'reuters']\n",
      "['athens', 'ap', 'olympic', 'team', 'first', 'games', 'quot', 'olympics', 'two', 'night']\n",
      "['company', 'oil', 'new', 'said', 'prices', 'reuters', 'inc', 'us', 'tuesday', 'more']\n"
     ]
    }
   ],
   "source": [
    "sorted_index = w_count.copy().argsort()[:,-10:][:,::-1]\n",
    "for class_i in range(len(sorted_index)):\n",
    "    top_neg_ngram = [vocab[i] for i in sorted_index[class_i]]\n",
    "    print(top_neg_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T23:51:03.977880Z",
     "start_time": "2020-03-20T23:46:49.797481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000100, alpha: 0.000100 ---> score:0.933333\n",
      "lr: 0.000100, alpha: 0.010000 ---> score:0.940000\n",
      "lr: 0.000100, alpha: 0.100000 ---> score:0.940000\n",
      "lr: 0.001000, alpha: 0.000100 ---> score:0.926667\n",
      "lr: 0.001000, alpha: 0.010000 ---> score:0.926667\n",
      "lr: 0.001000, alpha: 0.100000 ---> score:0.940000\n",
      "lr: 0.010000, alpha: 0.000100 ---> score:0.926667\n",
      "lr: 0.010000, alpha: 0.010000 ---> score:0.926667\n",
      "lr: 0.010000, alpha: 0.100000 ---> score:0.906667\n",
      "=================Best Hyperparameters=================\n",
      "lr: 0.0001,\talpha: 0.01,\tscore: 0.94\n",
      "----------------set dataset evaluation----------------\n",
      "Accuracy: 0.8788888888888889\n",
      "Precision: 0.8815954652603436\n",
      "Recall: 0.8788888888888889\n",
      "F1-Score: 0.8783657573979232\n"
     ]
    }
   ],
   "source": [
    "def get_F1(lr,alpha):\n",
    "    w, trl, devl = SGD(X_tr_tfidf, Y_tr, \n",
    "                         X_dev=X_dev_tfidf, \n",
    "                         Y_dev=Y_dev, \n",
    "                         num_classes=3,\n",
    "                         lr=lr, \n",
    "                         alpha=alpha, \n",
    "                         epochs=200,\n",
    "                         print_progress=False)\n",
    "    preds_dev = predict_class(X_dev_tfidf,w)\n",
    "    return accuracy_score(Y_dev,preds_dev)\n",
    "\n",
    "lrs = [0.0001,0.001,0.01]\n",
    "alphas = [0.0001,0.01,0.1]\n",
    "\n",
    "import itertools\n",
    "record = list()\n",
    "for lr,alpha in itertools.product(lrs,alphas):\n",
    "    score = get_F1(lr, alpha)\n",
    "    print(\"lr: %f, alpha: %f ---> score:%f\"%(lr,alpha,score))\n",
    "    record.append([lr,alpha,score])\n",
    "best = sorted(record,key=lambda l:l[-1],reverse=-1)[0]\n",
    "print(\"=================Best Hyperparameters=================\")\n",
    "print(\"lr: {},\\talpha: {},\\tscore: {}\".format(*best))\n",
    "print(\"----------------set dataset evaluation----------------\")\n",
    "lr,alpha=best[:-1]\n",
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_tfidf, Y_tr, \n",
    "                                             X_dev=X_dev_tfidf, \n",
    "                                             Y_dev=Y_dev,\n",
    "                                             num_classes=3,\n",
    "                                             lr=lr, \n",
    "                                             alpha=alpha, \n",
    "                                             epochs=200,\n",
    "                                             print_progress=False)\n",
    "\n",
    "preds_te = predict_class(X_te_tfidf,w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te.flatten(),preds_te))\n",
    "print('Precision:', precision_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te.flatten(),preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te.flatten(),preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:42.567569Z",
     "start_time": "2020-02-15T14:16:42.562560Z"
    }
   },
   "source": [
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |0.840   |0.837   |0.837   |\n",
    "| BOW-tfidf  |0.880   |0.877   |0.877   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "179px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
